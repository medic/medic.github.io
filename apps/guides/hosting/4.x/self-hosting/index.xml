<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Community Health Toolkit – Self Hosting in CHT 4.x</title><link>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/self-hosting/</link><description>Recent content in Self Hosting in CHT 4.x on Community Health Toolkit</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/self-hosting/index.xml" rel="self" type="application/rss+xml"/><item><title>Apps: Self Hosting in CHT 4.x - Single CouchDB Node</title><link>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/self-hosting/single-node/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/self-hosting/single-node/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;p>This for a single node CHT 4.x instance and is the recommended solution for small deployments. If you want a more powerful setup, check out &lt;a href="https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/self-hosting/multiple-nodes/">the 4.x multi-node install docs&lt;/a>.&lt;/p>
&lt;/div>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;p>Be sure you have followed &lt;a href="https://docs.communityhealthtoolkit.org/apps/guides/hosting/requirements/">the requirements document&lt;/a> including installing Docker and Docker Compose. This guide assumes you&amp;rsquo;re using the &lt;code>ubuntu&lt;/code> user and that it &lt;a href="https://askubuntu.com/a/477554">has &lt;code>sudo-less&lt;/code> access to Docker&lt;/a>.&lt;/p>
&lt;h2 id="directory-structure">Directory Structure&lt;/h2>
&lt;p>Create the following directory structure:&lt;/p>
&lt;pre tabindex="0">&lt;code>|-- /home/ubuntu/cht/
|-- compose/
|-- certs/
|-- couchdb/
|-- upgrade-service/
&lt;/code>&lt;/pre>&lt;p>By calling this &lt;code>mkdir&lt;/code> commands:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>mkdir -p /home/ubuntu/cht/&lt;span style="color:#ce5c00;font-weight:bold">{&lt;/span>compose,certs,upgrade-service,couchdb&lt;span style="color:#ce5c00;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol>
&lt;li>&lt;code>compose&lt;/code> - docker-compose files for cht-core and CouchDB&lt;/li>
&lt;li>&lt;code>certs&lt;/code> - TLS certificates directory&lt;/li>
&lt;li>&lt;code>upgrade-service&lt;/code> - where docker-compose file for the upgrade-service&lt;/li>
&lt;li>&lt;code>couchdb&lt;/code> - the path for the docker-compose file of the upgrade-service (not used in multi-node)&lt;/li>
&lt;/ol>
&lt;h2 id="download-required-docker-compose-files">Download required docker-compose files&lt;/h2>
&lt;p>The following 3 &lt;code>curl&lt;/code> commands download CHT version &lt;code>4.0.1&lt;/code> compose files, which you can change as needed. Otherwise, call:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> /home/ubuntu/cht/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -s -o ./compose/cht-core.yml https://staging.dev.medicmobile.org/_couch/builds_4/medic:medic:4.3.1/docker-compose/cht-core.yml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -s -o ./compose/cht-couchdb.yml https://staging.dev.medicmobile.org/_couch/builds_4/medic:medic:4.3.1/docker-compose/cht-couchdb.yml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -s -o ./upgrade-service/docker-compose.yml https://raw.githubusercontent.com/medic/cht-upgrade-service/main/docker-compose.yml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="prepare-environment-variables-file">Prepare Environment Variables file&lt;/h2>
&lt;p>Prepare a &lt;code>.env&lt;/code> file by running this code:&lt;/p>
&lt;pre tabindex="0">&lt;code>sudo apt install wamerican
uuid=$(uuidgen)
couchdb_secret=$(shuf -n7 /usr/share/dict/words --random-source=/dev/random | tr &amp;#39;\n&amp;#39; &amp;#39;-&amp;#39; | tr -d &amp;#34;&amp;#39;&amp;#34; | cut -d&amp;#39;-&amp;#39; -f1,2,3,4,5,6,7)
couchdb_password=$(shuf -n7 /usr/share/dict/words --random-source=/dev/random | tr &amp;#39;\n&amp;#39; &amp;#39;-&amp;#39; | tr -d &amp;#34;&amp;#39;&amp;#34; | cut -d&amp;#39;-&amp;#39; -f1,2,3,4,5,6,7)
cat &amp;gt; /home/ubuntu/cht/upgrade-service/.env &amp;lt;&amp;lt; EOF
CHT_COMPOSE_PROJECT_NAME=cht
COUCHDB_SECRET=${couchdb_secret}
DOCKER_CONFIG_PATH=/home/ubuntu/cht/upgrade-service
COUCHDB_DATA=/home/ubuntu/cht/couchdb
CHT_COMPOSE_PATH=/home/ubuntu/cht/compose
COUCHDB_USER=medic
COUCHDB_PASSWORD=${couchdb_password}
COUCHDB_UUID=${uuid}
EOF
&lt;/code>&lt;/pre>&lt;p>Note that secure passwords and UUIDs were generated on the first four calls and saved in the resulting &lt;code>.env&lt;/code> file.&lt;/p>
&lt;h2 id="launch-containers">Launch containers&lt;/h2>
&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
This section has the first use of &lt;code>docker compose&lt;/code>. This should work, but you may need to use the older style &lt;code>docker-compose&lt;/code> if you get an error &lt;code>docker: 'compose' is not a docker command.&lt;/code>.
&lt;/div>
&lt;p>To start your CHT instance, run the following&lt;/p>
&lt;pre tabindex="0">&lt;code>cd /home/ubuntu/cht/upgrade-service
docker compose up --detach
&lt;/code>&lt;/pre>&lt;p>Docker will start the upgrade service, which in turn pulls the required images and starts all the services as defined by the compose files in &lt;code>/home/ubuntu/cht/compose&lt;/code>.&lt;/p>
&lt;p>To follow the progress tail the log of the upgrade service container by running this:&lt;/p>
&lt;p>&lt;code>docker logs -f upgrade-service_cht-upgrade-service_1&lt;/code>&lt;/p>
&lt;p>To make sure everything is running correctly, call &lt;code>docker ps&lt;/code> and make sure that 7 CHT containers show:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>8c1c22d526f3 public.ecr.aws/s5s3h4s7/cht-nginx:4.0.1-4.0.1 &lt;span style="color:#4e9a06">&amp;#34;/docker-entrypoint.…&amp;#34;&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">17&lt;/span> minutes ago Up &lt;span style="color:#0000cf;font-weight:bold">8&lt;/span> minutes 0.0.0.0:80-&amp;gt;80/tcp, :::80-&amp;gt;80/tcp, 0.0.0.0:443-&amp;gt;443/tcp, :::443-&amp;gt;443/tcp cht_nginx_1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>f7b596be2721 public.ecr.aws/s5s3h4s7/cht-api:4.0.1-4.0.1 &lt;span style="color:#4e9a06">&amp;#34;/bin/bash /api/dock…&amp;#34;&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">17&lt;/span> minutes ago Up &lt;span style="color:#0000cf;font-weight:bold">8&lt;/span> minutes 5988/tcp cht_api_1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>029cd86ac721 public.ecr.aws/s5s3h4s7/cht-sentinel:4.0.1-4.0.1 &lt;span style="color:#4e9a06">&amp;#34;/bin/bash /sentinel…&amp;#34;&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">17&lt;/span> minutes ago Up &lt;span style="color:#0000cf;font-weight:bold">8&lt;/span> minutes cht_sentinel_1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>61ee1e0b377b public.ecr.aws/s5s3h4s7/cht-haproxy-healthcheck:4.0.1-4.0.1 &lt;span style="color:#4e9a06">&amp;#34;/bin/sh -c \&amp;#34;/app/ch…&amp;#34;&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">17&lt;/span> minutes ago Up &lt;span style="color:#0000cf;font-weight:bold">8&lt;/span> minutes cht_healthcheck_1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>87415a2d91ea public.ecr.aws/s5s3h4s7/cht-haproxy:4.0.1-4.0.1 &lt;span style="color:#4e9a06">&amp;#34;/entrypoint.sh -f /…&amp;#34;&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">17&lt;/span> minutes ago Up &lt;span style="color:#0000cf;font-weight:bold">8&lt;/span> minutes 5984/tcp cht_haproxy_1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>58454457467a public.ecr.aws/s5s3h4s7/cht-couchdb:4.0.1-4.0.1 &lt;span style="color:#4e9a06">&amp;#34;tini -- /docker-ent…&amp;#34;&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">17&lt;/span> minutes ago Up &lt;span style="color:#0000cf;font-weight:bold">8&lt;/span> minutes 4369/tcp, 5984/tcp, 9100/tcp cht_couchdb_1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>d01343658f3f public.ecr.aws/s5s3h4s7/cht-upgrade-service:latest &lt;span style="color:#4e9a06">&amp;#34;node /app/src/index…&amp;#34;&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">17&lt;/span> minutes ago Up &lt;span style="color:#0000cf;font-weight:bold">8&lt;/span> minutes upgrade-service-cht-upgrade-service-1
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This should show related to the CHT core are running&lt;/p>
&lt;ul>
&lt;li>cht_nginx&lt;/li>
&lt;li>cht_api&lt;/li>
&lt;li>cht_sentinel&lt;/li>
&lt;li>cht_couchdb&lt;/li>
&lt;li>cht_healthcheck&lt;/li>
&lt;li>cht_haproxy&lt;/li>
&lt;li>cht-upgrade-service&lt;/li>
&lt;/ul>
&lt;p>Take note of the &lt;code>STATUS&lt;/code> column and make sure no errors are displayed there. If any container is restarting or mentioning any other error, check the logs using the &lt;code>docker logs &amp;lt;container-name&amp;gt;&lt;/code> command.&lt;/p>
&lt;p>If all has gone well, nginx should now be listening at both port 80 and port 443. Port 80 has a permanent redirect to port 443, so you can only access the CHT using https.&lt;/p>
&lt;p>To login as the &lt;code>medic&lt;/code> user in the web app, you can find your password with this command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>grep COUCHDB_PASSWORD /home/ubuntu/cht/upgrade-service/.env &lt;span style="color:#000;font-weight:bold">|&lt;/span> cut -d&lt;span style="color:#4e9a06">&amp;#39;=&amp;#39;&lt;/span> -f2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="tls-certificates">TLS Certificates&lt;/h2>
&lt;p>See the &lt;a href="https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/adding-tls-certificates/">TLS Certificates page&lt;/a> for how to import your certificates.&lt;/p>
&lt;h2 id="upgrades">Upgrades&lt;/h2>
&lt;p>During upgrades, the CHT upgrade service updates the docker-compose files located in &lt;code>/home/ubuntu/cht/compose/&lt;/code>. This means that any and all changes made to the docker-compose files will be overwritten. If there is ever a need to make any changes to the docker-compose files, be sure to re-do them post upgrades or should consider implementing them outside of those docker-compose files.&lt;/p></description></item><item><title>Apps: Self Hosting in CHT 4.x - Multiple CouchDB Nodes on Docker Swarm</title><link>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/self-hosting/multiple-nodes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/self-hosting/multiple-nodes/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;p>The clustered multi-node hosting described below is recommended for deployments that need increased performance gains. These gains will increase the complexity of troubleshooting and decrease the ease ongoing maintenance.&lt;/p>
&lt;p>If you are unsure which deployment to use check out &lt;a href="https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/self-hosting/#recommendations-and-considerations">Self-hosting recommendations&lt;/a>.&lt;/p>
&lt;/div>
&lt;h3 id="about-clustered-deployments">About clustered deployments&lt;/h3>
&lt;p>In a clustered CHT setup, there are multiple CouchDB nodes responding to users. The ability to &lt;a href="https://en.wikipedia.org/wiki/Horizontal_scaling#Horizontal_(scale_out)_and_vertical_scaling_(scale_up)">horizontally scale&lt;/a> a CHT instance was added in version CHT 4.0.0. In this document we set up a three node CouchDB cluster. We require all three CouchDB nodes to be running and healthy before installing the CHT. Our healthcheck service determines the health of the CouchDB nodes and turns off the CHT if any single node is not functional.&lt;/p>
&lt;h3 id="nodes">Nodes&lt;/h3>
&lt;ul>
&lt;li>CHT Core (1x) - Core functionality of the CHT including API and sentinel&lt;/li>
&lt;li>CouchDB (3x) - 3 node CouchDB cluster&lt;/li>
&lt;/ul>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;h3 id="servers">Servers&lt;/h3>
&lt;p>Provision four Ubuntu servers (22.04 as of this writing) that meet our &lt;a href="https://docs.communityhealthtoolkit.org/apps/guides/hosting/requirements/">hosting requirements&lt;/a> including installing Docker and Docker Compose on all of them. This guide assumes you&amp;rsquo;re using the &lt;code>ubuntu&lt;/code> user, with a home directory of &lt;code>/home/ubuntu&lt;/code> and that it &lt;a href="https://askubuntu.com/a/477554">has &lt;code>sudo-less&lt;/code> access to Docker&lt;/a>.&lt;/p>
&lt;h3 id="network">Network&lt;/h3>
&lt;p>Make sure the following ports are open for all nodes:&lt;/p>
&lt;ul>
&lt;li>&lt;code>7946 TCP/UDP&lt;/code> - For Docker communication amongst nodes&lt;/li>
&lt;li>&lt;code>2377 TCP&lt;/code> - Docker cluster management communication&lt;/li>
&lt;li>&lt;code>4789 UDP&lt;/code> - Docker overlay network traffic&lt;/li>
&lt;li>&lt;code>ICMP&lt;/code> - For ping&lt;/li>
&lt;/ul>
&lt;p>As a security measure, be sure to restrict the IP addresses of the four nodes only to be able to connect to these ports.&lt;/p>
&lt;h2 id="create-an-overlay-network">Create an Overlay Network&lt;/h2>
&lt;p>To set up a private network that only the four nodes can use, we&amp;rsquo;ll use &lt;code>docker swarm&lt;/code>&amp;rsquo;s overlay network feature. You&amp;rsquo;ll first need to initialize the swarm on the CHT Core node and then join the swarm on each of the three CouchDB nodes.&lt;/p>
&lt;h3 id="cht-core-node">CHT Core node&lt;/h3>
&lt;p>Initialize swarm mode by running:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker swarm init
&lt;/code>&lt;/pre>&lt;p>This will output:&lt;/p>
&lt;div class="highlight">&lt;div style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code>&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
&lt;/span>&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
&lt;/span>&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
&lt;/span>&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
&lt;/span>&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
&lt;/span>&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>Swarm initialized: current node &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>ca7z1v4tm9q4kf9uimreqoauj&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span> is now a manager.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>To add a worker to this swarm, run the following command:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> docker swarm join --token &amp;lt;very-long-token-value&amp;gt; &amp;lt;main-server-private-ip&amp;gt;:2377
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>To add a manager to this swarm, run &lt;span style="color:#4e9a06">&amp;#39;docker swarm join-token manager&amp;#39;&lt;/span> and follow the instructions. &lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>
&lt;p>Then create overlay network by calling:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker network create --driver=overlay --attachable cht-net
&lt;/code>&lt;/pre>&lt;h3 id="couchdb-nodes">CouchDB nodes&lt;/h3>
&lt;p>On each of these three CouchDB nodes run the &lt;code>docker swarm join&lt;/code> command given to you in &lt;a href="#cht-core-node">line 4 above in &amp;ldquo;CHT Core node&amp;rdquo;&lt;/a>:&lt;/p>
&lt;pre>&lt;code>docker swarm join --token &amp;lt;very-long-token-value&amp;gt; &amp;lt;main-server-private-ip&amp;gt;:2377`
&lt;/code>&lt;/pre>
&lt;h3 id="confirm-swarm">Confirm swarm&lt;/h3>
&lt;p>Back on the CHT Core node, run &lt;code>docker node ls&lt;/code> and ensure you see 4 nodes listed as &lt;code>STATUS&lt;/code> of &lt;code>Ready&lt;/code> and &lt;code>AVAILABILITY&lt;/code> of &lt;code>Active&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>zolpxb5jpej8yiq9gcyv2nrdj * cht-core Ready Active Leader 20.10.23
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y9giir8c3ydifxvwozs3sn8vw couchdb1 Ready Active 20.10.23
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mi3vj0prd76djbvxms43urqiv couchdb2 Ready Active 20.10.23
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kcpxlci3jjjtm6xjz7v50ef7k couchdb3 Ready Active 20.10.23
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="cht-core-installation">CHT Core installation&lt;/h2>
&lt;p>Create the following directory structure:&lt;/p>
&lt;pre tabindex="0">&lt;code>|-- /home/ubuntu/cht/
|-- compose/
|-- certs/
|-- couchdb/
|-- upgrade-service/
&lt;/code>&lt;/pre>&lt;p>By calling this &lt;code>mkdir&lt;/code> commands:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>mkdir -p /home/ubuntu/cht/&lt;span style="color:#ce5c00;font-weight:bold">{&lt;/span>compose,certs,upgrade-service,couchdb&lt;span style="color:#ce5c00;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol>
&lt;li>&lt;code>compose&lt;/code> - docker-compose files for cht-core and CouchDB&lt;/li>
&lt;li>&lt;code>certs&lt;/code> - TLS certificates directory&lt;/li>
&lt;li>&lt;code>upgrade-service&lt;/code> - where docker-compose file for the upgrade-service&lt;/li>
&lt;li>&lt;code>couchdb&lt;/code> - the path for the docker-compose file of the upgrade-service (not used in multi-node)&lt;/li>
&lt;/ol>
&lt;h3 id="prepare-environment-variables-file">Prepare Environment Variables file&lt;/h3>
&lt;p>Prepare an &lt;code>.env&lt;/code> file by running this code:&lt;/p>
&lt;pre tabindex="0">&lt;code>sudo apt install wamerican
uuid=$(uuidgen)
couchdb_secret=$(shuf -n7 /usr/share/dict/words --random-source=/dev/random | tr &amp;#39;\n&amp;#39; &amp;#39;-&amp;#39; | tr -d &amp;#34;&amp;#39;&amp;#34; | cut -d&amp;#39;-&amp;#39; -f1,2,3,4,5,6,7)
couchdb_password=$(shuf -n7 /usr/share/dict/words --random-source=/dev/random | tr &amp;#39;\n&amp;#39; &amp;#39;-&amp;#39; | tr -d &amp;#34;&amp;#39;&amp;#34; | cut -d&amp;#39;-&amp;#39; -f1,2,3,4,5,6,7)
cat &amp;gt; /home/ubuntu/cht/upgrade-service/.env &amp;lt;&amp;lt; EOF
CHT_COMPOSE_PROJECT_NAME=cht
DOCKER_CONFIG_PATH=/home/ubuntu/cht/upgrade-service
CHT_COMPOSE_PATH=/home/ubuntu/cht/compose
COUCHDB_USER=medic
COUCHDB_PASSWORD=${couchdb_password}
COUCHDB_SERVERS=couchdb-1.local,couchdb-2.local,couchdb-3.local
EOF
&lt;/code>&lt;/pre>&lt;p>Note that secure passwords and UUIDs were generated on the first four calls and saved in the resulting &lt;code>.env&lt;/code> file.&lt;/p>
&lt;h3 id="download-compose-files">Download compose files&lt;/h3>
&lt;p>The following 2 &lt;code>curl&lt;/code> commands download CHT version &lt;code>4.0.1&lt;/code> compose files, which you can change as needed. Otherwise, call:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> /home/ubuntu/cht/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -s -o ./compose/cht-core.yml https://staging.dev.medicmobile.org/_couch/builds_4/medic:medic:4.3.1/docker-compose/cht-core.yml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -s -o ./upgrade-service/docker-compose.yml https://raw.githubusercontent.com/medic/cht-upgrade-service/main/docker-compose.yml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="compose-file-overrides">Compose file overrides&lt;/h4>
&lt;p>We need to override the &lt;code>networks:&lt;/code> in the two compose files we just created. Create the override file with this code:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>cat &amp;gt; /home/ubuntu/cht/compose/cluster-overrides.yml &lt;span style="color:#4e9a06">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">version: &amp;#39;3.9&amp;#39;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">networks:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> cht-net:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> driver: overlay
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> external: true
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="tls-certificates">TLS Certificates&lt;/h3>
&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
This section has the first use of &lt;code>docker compose&lt;/code>. This should work, but you may need to use the older style &lt;code>docker-compose&lt;/code> if you get an error &lt;code>docker: 'compose' is not a docker command&lt;/code>.
&lt;/div>
&lt;p>To ensure the needed docker volume is created, start the CHT Core services, which will intentionally all fail as the CouchDB nodes don&amp;rsquo;t exist. We&amp;rsquo;ll then ensure they&amp;rsquo;re all stopped with the &lt;code>docker kill&lt;/code> at the end. Note that this command has will &lt;code>sleep 120&lt;/code> (wait for 2 minutes) in hopes of&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> /home/ubuntu/cht/upgrade-service/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker compose up -d
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sleep &lt;span style="color:#0000cf;font-weight:bold">120&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker &lt;span style="color:#204a87">kill&lt;/span> &lt;span style="color:#204a87;font-weight:bold">$(&lt;/span>docker ps --quiet&lt;span style="color:#204a87;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With docker volume having been created, see the &lt;a href="https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/adding-tls-certificates/">TLS Certificates page&lt;/a> for how to import your certificates on the CHT Core node.&lt;/p>
&lt;h2 id="couchdb-installation-on-3-nodes">CouchDB installation on 3 nodes&lt;/h2>
&lt;p>Now that CHT Core is installed, we need to install CouchDB on the three nodes. Be sure all 3 nodes &lt;a href="#prerequisites">meet the prerequisites&lt;/a> before proceeding.&lt;/p>
&lt;h3 id="prepare-environment-variables-file-1">Prepare Environment Variables file&lt;/h3>
&lt;p>First, &lt;strong>on the CHT Core node&lt;/strong>, get your CouchDB password with this command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>grep COUCHDB_PASSWORD /home/ubuntu/cht/upgrade-service/.env &lt;span style="color:#000;font-weight:bold">|&lt;/span> cut -d&lt;span style="color:#4e9a06">&amp;#39;=&amp;#39;&lt;/span> -f2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now, &lt;strong>on all 3 CouchDB nodes&lt;/strong>, create an &lt;code>.env&lt;/code> file by running this code. You&amp;rsquo;ll need to replace &lt;code>PASSWORD-FROM-ABOVE&lt;/code> so it is the same on all three nodes:&lt;/p>
&lt;pre tabindex="0">&lt;code>sudo apt install wamerican
mkdir -p /home/ubuntu/cht/srv
uuid=$(uuidgen)
couchdb_secret=$(shuf -n7 /usr/share/dict/words --random-source=/dev/random | tr &amp;#39;\n&amp;#39; &amp;#39;-&amp;#39; | tr -d &amp;#34;&amp;#39;&amp;#34; | cut -d&amp;#39;-&amp;#39; -f1,2,3,4,5,6,7)
cat &amp;gt; /home/ubuntu/cht/.env &amp;lt;&amp;lt; EOF
CHT_COMPOSE_PROJECT_NAME=cht
COUCHDB_SECRET=${couchdb_secret}
COUCHDB_DATA=/home/ubuntu/cht/couchdb
COUCHDB_USER=medic
COUCHDB_PASSWORD=PASSWORD-FROM-ABOVE
COUCHDB_UUID=${uuid}
EOF
&lt;/code>&lt;/pre>&lt;p>Note that secure passwords and UUIDs were generated and saved in the resulting &lt;code>.env&lt;/code> file.&lt;/p>
&lt;h4 id="couchdb-node-1">CouchDB Node 1&lt;/h4>
&lt;p>Create &lt;code>/home/ubuntu/cht/docker-compose.yml&lt;/code> on Node 1 by running this code:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> /home/ubuntu/cht/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -s -o ./docker-compose.yml https://staging.dev.medicmobile.org/_couch/builds_4/medic:medic:4.3.1/docker-compose/cht-couchdb.yml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now create the override file to have Node 1 join the &lt;code>cht-net&lt;/code> overlay network we created above. As well, we&amp;rsquo;ll set some &lt;code>services:&lt;/code> specific overrides:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>cat &amp;gt; /home/ubuntu/cht/cluster-overrides.yml &lt;span style="color:#4e9a06">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">version: &amp;#39;3.9&amp;#39;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">services:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> couchdb:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> container_name: couchdb-1.local
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> environment:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> - &amp;#34;SVC_NAME=${SVC1_NAME:-couchdb-1.local}&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> - &amp;#34;CLUSTER_PEER_IPS=couchdb-2.local,couchdb-3.local&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">networks:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> cht-net:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> driver: overlay
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> external: true
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="couchdb-node-2">CouchDB Node 2&lt;/h4>
&lt;p>Like we did for Node 1, create &lt;code>/home/ubuntu/cht/docker-compose.yml&lt;/code> and the &lt;code>cluster-overrides.yml&lt;/code> file on Node 2 by running this code:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> /home/ubuntu/cht/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -s -o ./docker-compose.yml https://staging.dev.medicmobile.org/_couch/builds_4/medic:medic:4.3.1/docker-compose/cht-couchdb.yml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cat &amp;gt; /home/ubuntu/cht/cluster-overrides.yml &lt;span style="color:#4e9a06">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">version: &amp;#39;3.9&amp;#39;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">services:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> couchdb:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> container_name: couchdb-2.local
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> environment:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> - &amp;#34;SVC_NAME=couchdb-2.local&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> - &amp;#34;COUCHDB_SYNC_ADMINS_NODE=${COUCHDB_SYNC_ADMINS_NODE:-couchdb-1.local}&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">networks:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> cht-net:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> driver: overlay
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> external: true
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="couchdb-node-3">CouchDB Node 3&lt;/h4>
&lt;p>Finally, we&amp;rsquo;ll match Node 3 up with the others by running this code:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> /home/ubuntu/cht/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -s -o ./docker-compose.yml https://staging.dev.medicmobile.org/_couch/builds_4/medic:medic:4.3.1/docker-compose/cht-couchdb.yml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cat &amp;gt; /home/ubuntu/cht/cluster-overrides.yml &lt;span style="color:#4e9a06">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">version: &amp;#39;3.9&amp;#39;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">services:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> couchdb:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> container_name: couchdb-3.local
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> environment:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> - &amp;#34;SVC_NAME=couchdb-3.local&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> - &amp;#34;COUCHDB_SYNC_ADMINS_NODE=${COUCHDB_SYNC_ADMINS_NODE:-couchdb-1.local}&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">networks:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> cht-net:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> driver: overlay
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> external: true
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="starting-services">Starting Services&lt;/h2>
&lt;h3 id="couchdb-nodes-1">CouchDB Nodes&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>On each of the three CouchDB nodes starting with node 3, then 2 then 1. Be sure to wait until &lt;code>docker-compose&lt;/code> is finished running and has returned you to the command prompt before continuing to the next node:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> /home/ubuntu/cht
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker compose -f docker-compose.yml -f cluster-overrides.yml up -d
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Watch the logs and wait for everything to be up and running. You can run this on each node to watch the logs:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> /home/ubuntu/cht
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker compose logs --follow
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Nodes 2 and 3 should show output like &lt;code>couchdb is ready&lt;/code> after node 1 has started.&lt;/p>
&lt;p>Node 1 will show this when it has added all nodes:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>cht-couchdb-1.local-1 &lt;span style="color:#000;font-weight:bold">|&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">{&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;ok&amp;#34;&lt;/span>:true&lt;span style="color:#ce5c00;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cht-couchdb-1.local-1 &lt;span style="color:#000;font-weight:bold">|&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">{&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;all_nodes&amp;#34;&lt;/span>:&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;couchdb@couchdb-1.local&amp;#34;&lt;/span>,&lt;span style="color:#4e9a06">&amp;#34;couchdb@couchdb-2.local&amp;#34;&lt;/span>,&lt;span style="color:#4e9a06">&amp;#34;couchdb@couchdb-3.local&amp;#34;&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>,&lt;span style="color:#4e9a06">&amp;#34;cluster_nodes&amp;#34;&lt;/span>:&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;couchdb@couchdb-1.local&amp;#34;&lt;/span>,&lt;span style="color:#4e9a06">&amp;#34;couchdb@couchdb-2.local&amp;#34;&lt;/span>,&lt;span style="color:#4e9a06">&amp;#34;couchdb@couchdb-3.local&amp;#34;&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">]}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;h3 id="cht-core">CHT Core&lt;/h3>
&lt;p>Now that CouchDB is running on all the nodes, start the CHT Core:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> /home/ubuntu/cht/upgrade-service/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker compose -f docker-compose.yml -f ../compose/cluster-overrides.yml up -d
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To follow the progress tail the log of the upgrade service container by running:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> /home/ubuntu/cht/upgrade-service/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker compose logs --follow
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To make sure everything is running correctly, call &lt;code>docker ps&lt;/code> and make sure that 6 CHT containers show:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>8c1c22d526f3 public.ecr.aws/s5s3h4s7/cht-nginx:4.0.1-4.0.1 &lt;span style="color:#4e9a06">&amp;#34;/docker-entrypoint.…&amp;#34;&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">17&lt;/span> minutes ago Up &lt;span style="color:#0000cf;font-weight:bold">8&lt;/span> minutes 0.0.0.0:80-&amp;gt;80/tcp, :::80-&amp;gt;80/tcp, 0.0.0.0:443-&amp;gt;443/tcp, :::443-&amp;gt;443/tcp cht_nginx_1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>f7b596be2721 public.ecr.aws/s5s3h4s7/cht-api:4.0.1-4.0.1 &lt;span style="color:#4e9a06">&amp;#34;/bin/bash /api/dock…&amp;#34;&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">17&lt;/span> minutes ago Up &lt;span style="color:#0000cf;font-weight:bold">8&lt;/span> minutes 5988/tcp cht_api_1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>029cd86ac721 public.ecr.aws/s5s3h4s7/cht-sentinel:4.0.1-4.0.1 &lt;span style="color:#4e9a06">&amp;#34;/bin/bash /sentinel…&amp;#34;&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">17&lt;/span> minutes ago Up &lt;span style="color:#0000cf;font-weight:bold">8&lt;/span> minutes cht_sentinel_1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>61ee1e0b377b public.ecr.aws/s5s3h4s7/cht-haproxy-healthcheck:4.0.1-4.0.1 &lt;span style="color:#4e9a06">&amp;#34;/bin/sh -c \&amp;#34;/app/ch…&amp;#34;&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">17&lt;/span> minutes ago Up &lt;span style="color:#0000cf;font-weight:bold">8&lt;/span> minutes cht_healthcheck_1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>87415a2d91ea public.ecr.aws/s5s3h4s7/cht-haproxy:4.0.1-4.0.1 &lt;span style="color:#4e9a06">&amp;#34;/entrypoint.sh -f /…&amp;#34;&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">17&lt;/span> minutes ago Up &lt;span style="color:#0000cf;font-weight:bold">8&lt;/span> minutes 5984/tcp cht_haproxy_1 cht_couchdb_1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>d01343658f3f public.ecr.aws/s5s3h4s7/cht-upgrade-service:latest &lt;span style="color:#4e9a06">&amp;#34;node /app/src/index…&amp;#34;&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">17&lt;/span> minutes ago Up &lt;span style="color:#0000cf;font-weight:bold">8&lt;/span> minutes upgrade-service-cht-upgrade-service-1
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This should show related to the CHT core are running&lt;/p>
&lt;ul>
&lt;li>cht_nginx&lt;/li>
&lt;li>cht_api&lt;/li>
&lt;li>cht_sentinel&lt;/li>
&lt;li>cht_healthcheck&lt;/li>
&lt;li>cht_haproxy&lt;/li>
&lt;li>cht-upgrade-service&lt;/li>
&lt;/ul>
&lt;p>Take note of the &lt;code>STATUS&lt;/code> column and make sure no errors are displayed. If any container is restarting or mentioning any other error, check the logs using the &lt;code>docker logs &amp;lt;container-name&amp;gt;&lt;/code> command.&lt;/p>
&lt;p>If all has gone well, &lt;code>nginx&lt;/code> should now be listening at both port 80 and port 443. Port 80 has a permanent redirect to port 443, so you can only access the CHT using https.&lt;/p>
&lt;p>To login as the medic user in the web app, you can find your password with this command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>grep COUCHDB_PASSWORD /home/ubuntu/cht/upgrade-service/.env &lt;span style="color:#000;font-weight:bold">|&lt;/span> cut -d&lt;span style="color:#4e9a06">&amp;#39;=&amp;#39;&lt;/span> -f2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="upgrades">Upgrades&lt;/h2>
&lt;p>Upgrades are completely manual for the clustered setup right now. You have to go into each of the docker compose files and modify the image tag and take containers down and restart them.&lt;/p></description></item><item><title>Apps: Self Hosting in CHT 4.x - Multiple CouchDB Nodes on k3s on VMWare</title><link>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/self-hosting/self-hosting-k3s-multinode/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/self-hosting/self-hosting-k3s-multinode/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;p>This page covers an example k3s cluster setup on a VMware datacenter with vSphere 7+ for a national deployment across 50 counties capable of supporting 20,000+ CHWs concurrently. After setup, administrators should only add VMs to the cluster or deploy CHT Core projects to be orchestrated.&lt;/p>
&lt;/div>
&lt;h3 id="about-container-orchestration">About container orchestration&lt;/h3>
&lt;p>A container orchestrator helps easily allocate hardware resources spread across a datacenter. For national scale projects, or a deployments with a large number of CHT Core instances, Medic recommends a lightweight Kubernetes orchestrator called &lt;a href="https://docs.k3s.io/">k3s&lt;/a>. The orchestrator will:&lt;/p>
&lt;ul>
&lt;li>monitor resources across a group of virtual machines (aka &amp;ldquo;nodes&amp;rdquo;)&lt;/li>
&lt;li>place CHT Core projects where there is available resource&lt;/li>
&lt;li>migrate projects to spare resources if combined utilization is high or there are underlying issues.&lt;/li>
&lt;/ul>
&lt;p>Instead of provisioning one VM per CHT Core project, we will provision larger VMs and deploy multiple CHT Core projects on one VM, with each project receiving optional resource limitations, like CPU and RAM.&lt;/p>
&lt;p>In this example an orchestrator is deploying 50 CHT Core projects, one for each county. We will provision 9 large VMs and place 6 CHT Core projects on each VM. This allows for spare resources for failovers and lets the orchestrator decide on which VM projects live. Further, we get automated efficient use of datacenter resource utilization and avoids future manual allocations.&lt;/p>
&lt;h3 id="nodes">Nodes&lt;/h3>
&lt;p>We&amp;rsquo;ll be using two types of k3s nodes in this deployment:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://docs.k3s.io/installation/requirements#cpu-and-memory">HA control-plane&lt;/a> nodes - these enable high availability (HA) and provide access to kube API. These are containers running inside &lt;code>kube-system&lt;/code> namespace which are often associated with the control-plane. They include coreDNS, traefik (ingress), servicelb, VMware Cloud Provisioner Interface (CPI), and VMWare Container Storage Interface (CSI)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Agent or worker nodes - these run the CHT Core containers and projects. They will also run services that tie in networking and storage. VMware CSI-node will be running here which enables agents to mount volumes from VMware Virtual-SAN for block data storage. Agents will also run servicelb-traefik containers which allow the nodes to route traffic to correct projects and handle load-balancing and internal networking.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;h3 id="servers--virtual-machines">Servers / Virtual Machines&lt;/h3>
&lt;p>Provision 3 Ubuntu servers (22.04 as of this writing) that meet k3s specifications for &lt;a href="https://docs.k3s.io/installation/requirements#cpu-and-memory">HA etcd&lt;/a>&lt;/p>
&lt;p>As we&amp;rsquo;re provisioning an example deployment here for 50 counties and over 20,000 CHWs, the RAM, CPU and storage numbers will differ for you specific deployment.&lt;/p>
&lt;p>To support all 50 counties, provision 3 Ubuntu servers (22.04 as of this writing) with &lt;strong>4 vCPU and 8GB Ram&lt;/strong>. Ensure they also meet k3s specifications for &lt;a href="https://docs.k3s.io/installation/requirements#cpu-and-memory">HA etcd&lt;/a>.&lt;/p>
&lt;p>Provision 9 Ubuntu servers (again 22.04 as of this writing) for your k3s agent/worker servers. Each should have &lt;strong>48 vCPU, 192 GB Ram, and 50gb local storage&lt;/strong>.&lt;/p>
&lt;p>For any additional VMs you add to the k3s cluster, you will need to ensure networking, roles, and extra configuration parameters that are noted below are configured on the VM.&lt;/p>
&lt;p>To ensure your hardware is not over-provisioned, add more VMs to your k3s cluster when you want to deploy more CHT Core projects. This gives you flexibility of not needing to provision them initially as they can easily be added later.&lt;/p>
&lt;h3 id="network">Network&lt;/h3>
&lt;p>Ensure the above provisioned VMs:&lt;/p>
&lt;ul>
&lt;li>abide by &lt;a href="https://docs.k3s.io/installation/requirements#inbound-rules-for-k3s-server-nodes">Inbound Rules for k3s Server Nodes&lt;/a>&lt;/li>
&lt;li>If you&amp;rsquo;re using Ubuntu&amp;rsquo;s ufw, follow &lt;a href="https://docs.k3s.io/advanced#ubuntu--debian">firewall considerations for k3s on Ubuntu&lt;/a>&lt;/li>
&lt;li>are restricted to the IP addresses of the k3s nodes so only they can connect to the service ports&lt;/li>
&lt;/ul>
&lt;h3 id="add-roles-and-permissions-to-our-vms">Add Roles and Permissions to our VMs&lt;/h3>
&lt;p>Following the &lt;a href="https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/2.0/vmware-vsphere-csp-getting-started/GUID-0AB6E692-AA47-4B6A-8CEA-38B754E16567.html#GUID-0AB6E692-AA47-4B6A-8CEA-38B754E16567">vSphere docs&lt;/a>, first create the following vSphere roles in vSphere for Container Storage (CSN):&lt;/p>
&lt;ul>
&lt;li>CNS-VM&lt;/li>
&lt;li>CNS-DATASTORE&lt;/li>
&lt;li>CNS-SEARCH-AND-SPBM&lt;/li>
&lt;/ul>
&lt;p>Now, on the VM settings, we can apply these roles as described in the above document.&lt;/p>
&lt;p>Any provisioned VM in the previous step, should recieve CNS-VM role.
The top-level vCenter server will recieve CNS-SEARCH-AND-SPBM role.
Virtual-SAN should recieve CNS-DATASTORE.
And all servers should have the READONLY role (this may already be active)&lt;/p>
&lt;h3 id="enable-necessary-extra-parameters-on-all-vms">Enable Necessary Extra Parameters on all VMs&lt;/h3>
&lt;p>Following along the above document, we want to verify VM Hardware Version is 15 or greater, and that disk.EnableUUID parameter is configured.&lt;/p>
&lt;p>On each node, through vSphere Client (GUI):&lt;/p>
&lt;ol>
&lt;li>
&lt;p>disk.EnableUUID&lt;/p>
&lt;ol>
&lt;li>In the vSphere Client, right-click the VM and select Edit Settings.&lt;/li>
&lt;li>Click the VM Options tab and expand the Advanced menu.&lt;/li>
&lt;li>Click Edit Configuration next to Configuration Parameters.&lt;/li>
&lt;li>Configure the disk.EnableUUID parameter. If the parameter exists, make sure that its value is set to True. If the parameter is not present, add it and set its value to True.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Verify VM hardware version at 15 or higher, and upgrade if necessary&lt;/p>
&lt;ol>
&lt;li>In the vSphere Client, navigate to the virtual machine.&lt;/li>
&lt;li>Select Actions &amp;gt; Compatibility &amp;gt; Upgrade VM Compatibility.&lt;/li>
&lt;li>Click Yes to confirm the upgrade.&lt;/li>
&lt;li>Select a compatibility and click OK.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Add VMware Paravirtual SCSI storage controller to the VM&lt;/p>
&lt;ol>
&lt;li>In the vSphere Client, right-click the VM and select Edit Settings.&lt;/li>
&lt;li>On the Virtual Hardware tab, click the Add New Device button.&lt;/li>
&lt;li>Select SCSI Controller from the drop-down menu.&lt;/li>
&lt;li>Expand New SCSI controller and from the Change Type menu, select VMware Paravirtual.&lt;/li>
&lt;li>Click OK.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;h3 id="identify-vsphere-provider-ids-node-ids-and-datacenter-name">Identify vSphere Provider IDs, Node IDs, and datacenter name&lt;/h3>
&lt;p>Bootstrap parameters for k3s on VMware require UUID identification of each node that will join the cluster.&lt;/p>
&lt;p>For each of the provisioned VMs, you can navigate to the VM in vCenter interface and retrieve the UUID.&lt;/p>
&lt;p>Another method is to make the following calls to vCenter Server API. You may have a VPN that you connect to first before being able to access your vCenter GUI. These commands should be run from the same network that allows that access.&lt;/p>
&lt;p>When running the commands below, be sure to replace the placeholders with your own values:&lt;/p>
&lt;ul>
&lt;li>&lt;code>&amp;lt;vCenter_IP&amp;gt;&lt;/code>&lt;/li>
&lt;li>&lt;code>&amp;lt;USERNAME&amp;gt;&lt;/code>&lt;/li>
&lt;li>&lt;code>&amp;lt;PASSWORD&amp;gt;&lt;/code>&lt;/li>
&lt;li>&lt;code>&amp;lt;UUID_FROM_vCENTER&amp;gt;&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>And any others as well!&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Get an authentication-token:&lt;/p>
&lt;pre tabindex="0">&lt;code>curl -k -X POST https://&amp;lt;vCenter_IP&amp;gt;/rest/com/vmware/cis/session -u &amp;#39;&amp;lt;USERNAME&amp;gt;:&amp;lt;PASSWORD&amp;gt;&amp;#39;
ID=&amp;lt;UUID_FROM_vCENTER&amp;gt;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>List all your VMs and identify the VM-number that was provisioned earlier:&lt;/p>
&lt;pre tabindex="0">&lt;code>curl -k -X GET -H &amp;#34;vmware-api-session-id: $ID&amp;#34; https://&amp;lt;vCenter_IP&amp;gt;/api/vcenter/vm
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Retrieve your instance_uuid by first making a &lt;code>curl&lt;/code> call:&lt;/p>
&lt;pre tabindex="0">&lt;code>curl -k -X GET -H &amp;#34;vmware-api-session-id: $ID&amp;#34; https://&amp;lt;vCenter_IP&amp;gt;/api/vcenter/vm/vm-&amp;lt;number&amp;gt;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Inside the JSON response of the &lt;code>curl&lt;/code> call get the, &lt;code>instance_uuid&lt;/code>, in this case it&amp;rsquo;s &lt;code>215cc603-e8da-5iua-3333-a2402c05121&lt;/code>, but yours will be different:&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;#34;identity&amp;#34;:{&amp;#34;name&amp;#34;:&amp;#34;k3s_worker_node_4&amp;#34;,&amp;#34;instance_uuid&amp;#34;:&amp;#34;215cc603-e8da-5iua-3333-a2402c05121&amp;#34;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Retrieve your datacenter name, to be used in configuration files for VMware CSI and CPI&lt;/p>
&lt;pre tabindex="0">&lt;code>curl -k -X GET -H &amp;#34;vmware-api-session-id: $ID&amp;#34; https://&amp;lt;vCenter_IP&amp;gt;/rest/vcenter/datacenter
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ul>
&lt;p>You will want to save the &amp;ldquo;name&amp;rdquo; of your datacenter.&lt;/p>
&lt;ul>
&lt;li>Retrieve your cluster-id, to be used in config file for VMware CSI
&lt;pre tabindex="0">&lt;code>curl -k -X GET -H &amp;#34;vmware-api-session-id: $ID&amp;#34; https://&amp;lt;vCenter IP&amp;gt;/api/vcenter/cluster
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ul>
&lt;p>You can also use the &lt;a href="https://github.com/vmware/govmomi/blob/main/govc/README.md#binaries">govc cli tool&lt;/a> to retrieve this information:&lt;/p>
&lt;pre tabindex="0">&lt;code>export GOVC_INSECURE=1
export GOVC_URL=&amp;#39;https://&amp;lt;USERNAME&amp;gt;:&amp;lt;PASSWORD&amp;gt;@&amp;lt;vCenter_IP&amp;gt;
govc ls /
&amp;lt;datacenter-name&amp;gt;/vm \
&amp;lt;datacenter-name&amp;gt;/network \
&amp;lt;datacenter-name&amp;gt;/host \
&amp;lt;datacenter-name&amp;gt;/datastore
#To retrieve all Node VMs
govc ls /&amp;lt;datacenter-name&amp;gt;/vm \
&amp;lt;datacenter-name&amp;gt;/vm/&amp;lt;vm-name1&amp;gt; \
&amp;lt;datacenter-name&amp;gt;/vm/&amp;lt;vm-name2&amp;gt; \
&amp;lt;datacenter-name&amp;gt;/vm/&amp;lt;vm-name3&amp;gt; \
&amp;lt;datacenter-name&amp;gt;/vm/&amp;lt;vm-name4&amp;gt; \
&amp;lt;datacenter-name&amp;gt;/vm/&amp;lt;vm-name5&amp;gt;
&lt;/code>&lt;/pre>&lt;h2 id="install-k3s">Install k3s&lt;/h2>
&lt;h3 id="first-control-plane-vm">First Control-Plane VM&lt;/h3>
&lt;p>SSH into your first control-plane VM that was provisioned and configured above and &lt;a href="https://docs.docker.com/engine/install/ubuntu/">install docker&lt;/a>.&lt;/p>
&lt;p>For k3s version compatibiltiy with vCenter and vMware CPI/CSI, we will need to use k3s v1.25, cpi v1.25, and csi v2.7.2 per the &lt;code>curl&lt;/code> call below.&lt;/p>
&lt;p>Run the following CLI command inside the control-plane VM, filling out these two specific values:&lt;/p>
&lt;ul>
&lt;li>&lt;code>&amp;lt;TOKEN&amp;gt;&lt;/code>: Please generate a token ID, and save it. This will be required for the entirety of the k3s cluster existence and required to add additional servers to the k3s cluster&lt;/li>
&lt;li>&lt;code>&amp;lt;VM_UUID&amp;gt;&lt;/code>: This was the UUID for this specific VM that we identified earlier&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=&amp;#34;644&amp;#34; \
INSTALL_K3S_EXEC=&amp;#34;server&amp;#34; INSTALL_K3S_VERSION=&amp;#34;v1.25.14+k3s1&amp;#34; sh -s - \
--docker --token &amp;lt;TOKEN&amp;gt; \
--cluster-init --disable-cloud-controller \
--kubelet-arg=&amp;#34;cloud-provider=external&amp;#34; \
--kubelet-arg=&amp;#34;provider-id=vsphere://&amp;lt;VM_UUID&amp;gt;&amp;#34;
&lt;/code>&lt;/pre>&lt;h3 id="second-and-third-control-plane-vms">Second and third Control-Plane VMs&lt;/h3>
&lt;p>SSH into your second/third control-plane VM.&lt;/p>
&lt;p>Please fill out these values below and run the cli command:&lt;/p>
&lt;ul>
&lt;li>&lt;code>&amp;lt;TOKEN&amp;gt;&lt;/code>: Required to be the same token you used in the first control-plane setup&lt;/li>
&lt;li>&lt;code>&amp;lt;CONTROL_PLANE_1_IP&amp;gt;&lt;/code>: This is the IP of the first control-plane server you setup, and allows this second server to discover the initial one.&lt;/li>
&lt;li>&lt;code>&amp;lt;VM_UUID&amp;gt;&lt;/code>: This is the UUID for this second VM that we identified earlier. This will be different than the one you used for control plane 1.&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=&amp;#34;644&amp;#34; \
INSTALL_K3S_EXEC=&amp;#34;server&amp;#34; INSTALL_K3S_VERSION=&amp;#34;v1.25.14+k3s1&amp;#34; sh -s \
--docker --token &amp;lt;TOKEN&amp;gt; \
--server https://&amp;lt;CONTROL_PLANE_1_IP:6443 \
--disable-cloud-controller --kubelet-arg=&amp;#34;cloud-provider=external&amp;#34; \
--kubelet-arg=&amp;#34;provider-id=vsphere://&amp;lt;VM_UUID&amp;gt;&amp;#34;
&lt;/code>&lt;/pre>&lt;p>You can verify your cluster is working by running this command from inside your control plane VM:&lt;/p>
&lt;pre tabindex="0">&lt;code>/usr/local/bin/k3s kubectl get nodes -o wide
&lt;/code>&lt;/pre>&lt;h3 id="agentworker-vms">Agent/Worker VMs&lt;/h3>
&lt;p>Now we will add our k3s agent/worker servers that will handle cht-core projects, workloads, and containers. This process is the same for any additional Agent/Worker servers you want to add to your k3s cluster.&lt;/p>
&lt;p>Ensure that the appropriate roles, and extra configuration parameters are set correctly.&lt;/p>
&lt;p>Please fill out these values before running the command:&lt;/p>
&lt;ul>
&lt;li>&lt;code>&amp;lt;TOKEN&amp;gt;&lt;/code>: Required to be the same token you used above&lt;/li>
&lt;li>&lt;code>&amp;lt;CONTROL_PLANE_IP&amp;gt;&lt;/code>: The IP of one of the control plane servers you set up above&lt;/li>
&lt;li>&lt;code>&amp;lt;VM_UUID&amp;gt;&lt;/code>: This is the UUID of this VM that we are adding as an agent/worker server in our k3s cluster&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=&amp;#34;644&amp;#34; \
INSTALL_K3S_EXEC=&amp;#34;agent&amp;#34; INSTALL_K3S_VERSION=&amp;#34;v1.25.14+k3s1&amp;#34; sh -s - \
--docker --token &amp;lt;TOKEN&amp;gt; \
--server https://&amp;lt;CONTROL_PLANE_IP&amp;gt;:6443 \
--kubelet-arg=&amp;#34;cloud-provider=external&amp;#34; \
--kubelet-arg=&amp;#34;provider-id=vsphere://&amp;lt;VM_UUID&amp;gt;&amp;#34;
&lt;/code>&lt;/pre>&lt;h2 id="deploy-vmware-cloud-provisioner-interface-cpi-to-your-k3s-cluster">Deploy VMware Cloud Provisioner Interface (CPI) to your k3s cluster&lt;/h2>
&lt;p>SSH into one of your control plane servers.
Download the template for CPI, ensure you are aware of your current working directory. This will be the location where the CPI template is saved.&lt;/p>
&lt;pre tabindex="0">&lt;code>pwd
wget https://raw.githubusercontent.com/kubernetes/cloud-provider-vsphere/release-1.25/releases/v1.25/vsphere-cloud-controller-manager.yaml
&lt;/code>&lt;/pre>&lt;p>Modify the vsphere-cloud-controller-manager.yaml file downloaded above and update vCenter Server information.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Add your &lt;code>&amp;lt;vCenter_IP&amp;gt;&lt;/code> and &lt;code>&amp;lt;USERNAME&amp;gt;&lt;/code>, &lt;code>&amp;lt;PASSWORD&amp;gt;&lt;/code> to the section below inside that yaml:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: v1
kind: Secret
metadata:
name: vsphere-cloud-secret
labels:
vsphere-cpi-infra: secret
component: cloud-controller-manager
namespace: kube-system
# NOTE: this is just an example configuration, update with real values based on your environment
stringData:
&amp;lt;vCenter_IP&amp;gt;.username: &amp;#34;&amp;lt;USERNAME&amp;gt;&amp;#34;
&amp;lt;vCenter_IP&amp;gt;.password: &amp;#34;&amp;lt;PASSWORD&amp;gt;&amp;#34;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Please add your &lt;code>&amp;lt;vCenter_IP&amp;gt;&lt;/code> and &lt;code>&amp;lt;USERNAME&amp;gt;&lt;/code>, &lt;code>&amp;lt;PASSWORD&amp;gt;&lt;/code> and &lt;code>&amp;lt;Datacenter_name_retrieved_earlier&amp;gt;&lt;/code> to the ConfigMap section inside that yaml.&lt;/p>
&lt;p>&lt;strong>Note:&lt;/strong> If your vCenter actively uses https with valid certificates, then inside the &lt;code>global:&lt;/code> stanza, you will want to set &lt;code>insecureFlag: false&lt;/code>. Most set-ups will want this to remain true with&lt;code>insecureFlag: true&lt;/code> .&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: v1
kind: ConfigMap
metadata:
name: vsphere-cloud-config
labels:
vsphere-cpi-infra: config
component: cloud-controller-manager
namespace: kube-system
data:
# NOTE: this is just an example configuration, update with real values based on your environment
vsphere.conf: |
# Global properties in this section will be used for all specified vCenters unless overriden in VirtualCenter section.
global:
port: 443
# set insecureFlag to true if the vCenter uses a self-signed cert
insecureFlag: true
# settings for using k8s secret
secretName: vsphere-cloud-secret
secretNamespace: kube-system
# vcenter section
vcenter:
my-vc-name:
server: &amp;lt;vCenter_IP&amp;gt;
user: &amp;lt;USERNAME&amp;gt;
password: &amp;lt;PASSWORD&amp;gt;
datacenters:
- &amp;lt;Datacenter_name_retrieved_earlier&amp;gt;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Deploy the template!&lt;/p>
&lt;pre tabindex="0">&lt;code>/usr/local/bin/k3s kubectl -n kube-system apply -f vsphere-cloud-controller-manager.yaml
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Verify CPI containers are running:&lt;/p>
&lt;pre tabindex="0">&lt;code>/usr/local/bin/k3s kubectl -n kube-system get pods -o wide
/usr/local/bin/k3s kubectl -n kube-system logs vsphere-cloud-controller-manager-&amp;lt;id&amp;gt;
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ol>
&lt;p>You will see 3 vsphere-cloud-controller-manager pods running, one per control-plane server.&lt;/p>
&lt;p>Take a peak at all 3 vsphere-controller-manager pods logs to ensure nothing is immediately erring. Common errors are using the incorrect datacenter name, UUIDs for VMs in the k3s curl command, or invalid credentials in the configmap and secrets resources created in step 2 above. If one of these errors is displaying in the log, you will want to delete the deployment (in step 3 above, replace &lt;code>apply&lt;/code> with &lt;code>delete&lt;/code>, edit the yaml and re-deploy (run step 3 again).&lt;/p>
&lt;h2 id="deploy-vmware-container-storage-interface-csi-to-your-k3s-cluster">Deploy VMware Container Storage Interface (CSI) to your k3s cluster&lt;/h2>
&lt;p>Follow the &lt;a href="https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/2.0/vmware-vsphere-csp-getting-started/GUID-A1982536-F741-4614-A6F2-ADEE21AA4588.html">VMware documentation for CSI&lt;/a> with these steps:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Run the following command from inside a control-plane server:&lt;/p>
&lt;pre tabindex="0">&lt;code>/usr/local/bin/k3s kubectl create namespace vmware-system-csi
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Taint your control-lane node servers by running the following command. This taint may already exist, if so, thats okay. Please replace &lt;code>&amp;lt;CONTROL_PLANE_SERVER&amp;gt;&lt;/code> with each of your control plane servers.&lt;/p>
&lt;pre tabindex="0">&lt;code>You can retrieve the names by running `/usr/local/bin/k3s kubectl get nodes -o wide`
/usr/local/bin/k3s kubectl taint node &amp;lt;CONTROL_PLANE_SERVER&amp;gt; node-role.kubernetes.io/control-plane=:NoSchedule
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Create kubernetes secret, which will map authentication credentials and datacenter name to CSI containers. First, create a file &lt;code>/etc/kubernetes/csi-vsphere.conf&lt;/code>. Be sure to replace &lt;code>&amp;lt;vCenter_IP&amp;gt;&lt;/code>, &lt;code>&amp;lt;USERNAME&amp;gt;&lt;/code>, &lt;code>&amp;lt;PASSWORD&amp;gt;&lt;/code> , &lt;code>&amp;lt;true_or_false&amp;gt;&lt;/code>, &lt;code>&amp;lt;PORT&amp;gt;&lt;/code> , &lt;code>&amp;lt;datacenter1-path&amp;gt;&lt;/code> and &lt;code>&amp;lt;datacenter1-path&amp;gt;&lt;/code> with your values:&lt;/p>
&lt;pre tabindex="0">&lt;code>[Global]
cluster-id = &amp;#34;&amp;lt;cluster-id&amp;gt;&amp;#34;
[VirtualCenter &amp;#34;&amp;lt;vCenter_IP&amp;gt;&amp;#34;]
insecure-flag = &amp;#34;&amp;lt;true_or_false&amp;gt;&amp;#34;
user = &amp;#34;&amp;lt;USERNAME&amp;gt;&amp;#34;
password = &amp;#34;&amp;lt;PASSWORD&amp;gt;&amp;#34;
port = &amp;#34;&amp;lt;PORT&amp;gt;&amp;#34;
datacenters = &amp;#34;&amp;lt;datacenter1-path&amp;gt;, &amp;lt;datacenter2-path&amp;gt;, ...&amp;#34;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Create the secret resource in the namespace we created in step 1 by running the following command in the same directory you created the csi-vsphere.conf file:&lt;/p>
&lt;pre tabindex="0">&lt;code>/usr/local/bin/k3s kubectl create secret generic vsphere-config-secret --from-file=csi-vsphere.conf --namespace=vmware-system-csi
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Download the &lt;a href="https://raw.githubusercontent.com/kubernetes-sigs/vsphere-csi-driver/v2.7.2/manifests/vanilla/vsphere-csi-driver.yaml">vSphere CSI v2.7.2 template&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>There is one minor edit, typically found on line 217-218, under the deployment specification for vsphere-csi-controller.&lt;/p>
&lt;p>Before edit (original value)&lt;/p>
&lt;pre tabindex="0">&lt;code> nodeSelector:
node-role.kubernetes.io/control-plane: &amp;#34;&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Please add &lt;code>true&lt;/code> as the value for this key, seen below:&lt;/p>
&lt;pre tabindex="0">&lt;code> nodeSelector:
node-role.kubernetes.io/control-plane: &amp;#34;true&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Now, let&amp;rsquo;s deploy VMware CSI by running the following command:&lt;/p>
&lt;pre tabindex="0">&lt;code>/usr/local/bin/k3s kubectl -n vmware-system-csi apply -f vsphere-csi-driver.yaml
&lt;/code>&lt;/pre>&lt;p>Follow the &lt;a href="https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/2.0/vmware-vsphere-csp-getting-started/GUID-54BB79D2-B13F-4673-8CC2-63A772D17B3C.html">verification steps seen here in Step 2 of Procedure&lt;/a>&lt;/p>
&lt;h3 id="create-storageclass-in-k3s-cluster">Create StorageClass in k3s cluster&lt;/h3>
&lt;p>We&amp;rsquo;ll need to create a global &lt;a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">StorageClass&lt;/a> resource in our k3s cluster, so CHT Core deployments will be able to ask for persistent storage volumes from the k3s cluster.&lt;/p>
&lt;p>Inside one of the control-plane servers, please create a file &lt;code>vmware-storageclass.yaml&lt;/code> with the following contents:&lt;/p>
&lt;pre tabindex="0">&lt;code>kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
name: vmware-sc
annotations:
storageclass.kubernetes.io/is-default-class: &amp;#34;true&amp;#34;
provisioner: csi.vsphere.vmware.com
parameters:
csi.storage.k8s.io/fstype: &amp;#34;ext4&amp;#34; #Optional Parameter
&lt;/code>&lt;/pre>&lt;p>Deploy this template to the k3s cluster via:&lt;/p>
&lt;pre tabindex="0">&lt;code>/usr/local/bin/k3s kubectl apply -f vmware-storageclass.yaml
&lt;/code>&lt;/pre>&lt;h2 id="deploying-a-cht-core-project-to-your-new-k3s-cluster-running-on-vmware">Deploying a CHT-Core Project to your new k3s Cluster running on VMware&lt;/h2>
&lt;p>This step will neatly fit into helm chart configurations, but here are the manual steps for time being.&lt;/p>
&lt;p>Your persistent volume (PVC) template for all CouchDB&amp;rsquo;s should be as shown below. Note the &lt;code>storageClassName&lt;/code> parameter should be identical to the &lt;code>storageClass&lt;/code> we deployed earlier:&lt;/p>
&lt;pre tabindex="0">&lt;code># Source: cht-chart/templates/couchdb-n-claim0-persistentvolumeclaim.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
labels:
cht.service: couchdb-1-claim0
name: couchdb-1-claim0
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 4Gi
storageClassName: vmware-sc
status: {}
&lt;/code>&lt;/pre>&lt;h2 id="kubernetes-concepts">Kubernetes Concepts&lt;/h2>
&lt;p>Here are links to docs surrounding the kubernetes concepts that we use in a cht-core project deployed to a k3s cluster.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment&lt;/a> - This is the main kubernetes resource that contains information regarding all the cht services that will be deployed.&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/configuration/configmap/">ConfigMaps&lt;/a> - This contains configuration files, or credentials that containers can retrieve. If you edit the configmap, you should delete containers, which will trigger a new container to download your new edits to any configrations for that service&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/security/service-accounts/">ServiceAccounts&lt;/a> - This is used by the upgrade-service that is running inside the cht-core pods (as a container titled upgrade-service). This serviceAccount restricts the upgrade-service from interacting with any other cht-core projects outside of its namespace, and gives the upgrade-service permissions to talk to kubernetes API to upgrade container images when a CHT ADMIN clicks &lt;em>upgrade&lt;/em> through the Admin interface.&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress&lt;/a> - This is what forwards traffic to a particular project or pods. In most use-cases, there is an nginx deployed outside of the k3s cluster than contains DNS entries for existing projects, and contains a proxy_pass parameter to send traffic based on host header to any of the k3s server IPs. Inside the k3s cluster, the traefik container and servicelb-traefik containers in kube-system namespace will handle forwarding traffic to the correct cht-core containers based on url&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volume Claim&lt;/a> - This is where our project data will be stored. Important to ensure you have configured this correctly, with retain policies intact so the data is not deleted if the project is removed. It&amp;rsquo;s also vital to ensure you have a backup policy either set-up in VMware vCenter GUI or you have configured the csi-snapshotter that comes with vSphere CSI.&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/">Services&lt;/a> - This is utilized for CouchDB nodes to discover each other through DNS rather than internal IPs, which can change. This is also used in the COUCH_URL so API containers can discover where CouchDB is running.&lt;/li>
&lt;/ul></description></item></channel></rss>