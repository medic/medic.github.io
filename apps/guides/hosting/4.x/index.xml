<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Community Health Toolkit – 4.x</title><link>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/</link><description>Recent content in 4.x on Community Health Toolkit</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/index.xml" rel="self" type="application/rss+xml"/><item><title>Apps: Migration from CHT 3.x to CHT 4.x</title><link>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/data-migration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/data-migration/</guid><description>
&lt;p>The hosting architecture differs entirely between CHT-Core 3.x and CHT-Core 4.x. Migrating data from an existing instance running CHT 3.x requires a few manual steps.
This guide will present the required steps while using a migration helping tool, called &lt;code>couchdb-migration&lt;/code>. This tool interfaces with CouchDb, to update shard maps and database metadata.
By the end of this guide, your CHT-Core 3.x CouchDb will be down and CHT-Core 4.x ready to be used.
Using this tool is not required, and the same result can be achieved by calling CouchDb endpoints directly. &lt;a href="https://docs.couchdb.org/en/stable/cluster/sharding.html#moving-a-shard">Consult CouchDB documentation for details about moving shards&lt;/a>.&lt;/p>
&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
If after upgrading you get an error, &lt;code>Cannot convert undefined or null to object&lt;/code> - please see &lt;a href="https://github.com/medic/cht-core/issues/8040">issue #8040&lt;/a> for a work around. This only affects CHT 4.0.0, 4.0.1, 4.1.0 and 4.1.1. It was fixed in CHT 4.2.0.
&lt;/div>
&lt;h3 id="1-install-cht-data-migration-tool">1. Install CHT data migration tool&lt;/h3>
&lt;p>Open your terminal and run these commands. They will create a new directory, download a docker compose file and download the required docker image.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>mkdir -p ~/couchdb-migration/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> ~/couchdb-migration/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -s -o ./docker-compose.yml https://raw.githubusercontent.com/medic/couchdb-migration/main/docker-compose.yml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker-compose up
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For the following steps, the tool needs access to your CouchDb installation. To allow this access, you will need to provide a URL to your CouchDB installation that includes authentication.
If your installation exposes a different port for CouchDb cluster API endpoints, please export that port.
If running against an installation of &lt;code>MedicOS&lt;/code>, please make sure that the protocol of the URL is &lt;code>https&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">export&lt;/span> &lt;span style="color:#000">COUCH_URL&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>http&lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>s&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>://&amp;lt;authentication&amp;gt;@&amp;lt;host-ip&amp;gt;:&amp;lt;port&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For simplicity, you could store these required values in an &lt;code>.env&lt;/code> file:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>cat &amp;gt; &lt;span style="color:#4e9a06">${&lt;/span>&lt;span style="color:#000">HOME&lt;/span>&lt;span style="color:#4e9a06">}&lt;/span>/couchdb-migration/.env &lt;span style="color:#4e9a06">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">COUCH_URL=http(s)://&amp;lt;authentication&amp;gt;@&amp;lt;host-ip&amp;gt;:&amp;lt;port&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="2-prepare-cht-core-3x-installation-for-upgrading">2. Prepare CHT-Core 3.x installation for upgrading&lt;/h3>
&lt;p>Backup your data! If you encounter any problems executing the instructions of this guide, you should be able to restore your CHT 3X instance using the backup data.
&lt;a href="https://docs.communityhealthtoolkit.org/apps/guides/hosting/3.x/self-hosting/#backup">Consult information about backups for details&lt;/a>.
Ensure no changes happen to your CouchDB data in your CHT 3.x server after you have begun the migration process.&lt;/p>
&lt;p>To minimize downtime when upgrading, it&amp;rsquo;s advised to prepare the 3.x installation for the 4.x upgrade, and pre-index all views that are required by 4.x.&lt;/p>
&lt;p>The migration tool provides a command which will download all 4.x views to your 3.x CouchDb, and initiate view indexing. &lt;code>&amp;lt;desired CHT version&amp;gt;&lt;/code> is any version at or above &lt;code>4.0.0&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> ~/couchdb-migration/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker-compose run couch-migration pre-index-views &amp;lt;desired CHT version&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once view indexing is finished, proceed with the next step.&lt;/p>
&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
If this step is omitted, 4.x API will fail to respond to requests until all views are indexed. Depending on the size of the database, this could take many hours, or even days.
&lt;/div>
&lt;h3 id="3-save-existent-couchdb-configuration">3. Save existent CouchDb configuration&lt;/h3>
&lt;p>Some CouchDb configuration values must be ported from existent CouchDb to the 4.x installation. Store them in a safe location before shutting down 3.x CouchDb.
Use the migration tool to obtain these values:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> ~/couchdb-migration/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker-compose run couch-migration get-env
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="a-couchdb-secret">a) CouchDB secret&lt;/h5>
&lt;p>Used in encrypting all CouchDb passwords and session tokens.&lt;/p>
&lt;h5 id="b-couchdb-server-uuid">b) CouchDb server uuid&lt;/h5>
&lt;p>Used in generating replication checkpointer documents, which track where replication progress between every client and the server, and ensure that clients don&amp;rsquo;t re-download or re-upload documents.&lt;/p>
&lt;h3 id="4-locate-and-make-a-copy-of-your-couchdb-data-folder">4. Locate and make a copy of your CouchDb Data folder&lt;/h3>
&lt;p>a) If running in MedicOS, &lt;a href="https://docs.communityhealthtoolkit.org/apps/guides/hosting/3.x/self-hosting/#backup">CouchDb data folder&lt;/a> can be found at &lt;code>/srv/storage/medic-core/couchdb/data&lt;/code>.&lt;/p>
&lt;p>b) If running a custom installation of CouchDb, data would be typically stored at &lt;code>/opt/couchdb/data&lt;/code>.&lt;/p>
&lt;h3 id="5-stop-your-3x-couchdb--cht-core-installation-and-launch-4x-couchdb-installation">5. Stop your 3.x CouchDb / CHT-Core installation and launch 4.x CouchDb installation&lt;/h3>
&lt;p>Depending on your project scalability needs and technical possibilities, you must decide whether you will deploy CouchDb in a single node or in a cluster with multiple nodes.
Please consult this guide about clustering and horizontal scalability to make an informed decision. &lt;insert link>&lt;/p>
&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
You can start with single node and then change to a cluster. This involves running the migration tool again to distribute shards from the existent node to the new nodes.
&lt;/div>
&lt;p>Depending on your choice, follow the instructions that match your deployment below:&lt;/p>
&lt;h4 id="single-node">Single node&lt;/h4>
&lt;p>a) Download 4.x single-node CouchDb docker-compose file:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>mkdir -p ~/couchdb-single/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> ~/couchdb-single/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -s -o ./docker-compose.yml https://staging.dev.medicmobile.org/_couch/builds_4/medic:medic:&amp;lt;desired CHT version&amp;gt;/docker-compose/cht-couchdb.yml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>a) Make a copy of the 3.x CouchDb data folder from &lt;strong>step 4&lt;/strong>.&lt;/p>
&lt;p>b) Set the correct environment variables:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>cat &amp;gt; &lt;span style="color:#4e9a06">${&lt;/span>&lt;span style="color:#000">HOME&lt;/span>&lt;span style="color:#4e9a06">}&lt;/span>/couchdb-single/.env &lt;span style="color:#4e9a06">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">COUCHDB_USER=&amp;lt;admin&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">COUCHDB_PASSWORD=&amp;lt;password&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">COUCHDB_SECRET=&amp;lt;COUCHDB_SECRET from step 3&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">COUCHDB_UUID=&amp;lt;COUCHDB_UUID from step 3&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">COUCHDB_DATA=&amp;lt;absolute path to folder created in step 5.a&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>c) Start 4.x CouchDb.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> ~/couchdb-single/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker-compose up -d
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>d) Update &lt;code>couchdb-migration&lt;/code> environment variables. Depending on your setup, it&amp;rsquo;s possible you will need to update &lt;code>CHT_NETWORK&lt;/code> and &lt;code>COUCH_URL&lt;/code> to match the newly started 4.x CouchDb.
From this point on, the &lt;code>couchdb-migration&lt;/code> container should connect to the same docker network as your CouchDb installation, in order to access APIs that are only available on protected ports. Correctly setting &lt;code>CHT_NETWORK&lt;/code> is required for the next steps to succeed.
To get the correct &lt;code>docker-network-name&lt;/code> and &lt;code>docker-service-name&lt;/code>, you can use &lt;code>docker network ls&lt;/code> to list all networks and &lt;code>docker network inspect &amp;lt;docker-network-name&amp;gt;&lt;/code> to get the name of the CouchDb container that exists in this network.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>cat &amp;gt; &lt;span style="color:#4e9a06">${&lt;/span>&lt;span style="color:#000">HOME&lt;/span>&lt;span style="color:#4e9a06">}&lt;/span>/couchdb-migration/.env &lt;span style="color:#4e9a06">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">CHT_NETWORK=&amp;lt;docker-network-name&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">COUCH_URL=http://&amp;lt;authentication&amp;gt;@&amp;lt;docker-container-name&amp;gt;:&amp;lt;port&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>e) Check that &lt;code>couchdb-migration&lt;/code> can connect to the CouchDb instance and that CouchDb is running. You&amp;rsquo;ll know it is working when the &lt;code>docker-compose&lt;/code> call exits without errors and logs &lt;code>CouchDb is Ready&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> ~/couchdb-migration/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker-compose run couch-migration check-couchdb-up
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>f) Change metadata to match the new CouchDb node&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> ~/couchdb-migration/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker-compose run couch-migration move-node
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>g) Run the &lt;code>verify&lt;/code> command to check whether the migration was successful.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>docker-compose run couch-migration verify
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If all checks pass, you should see a message &lt;code>Migration verification passed&lt;/code>. It is then safe to proceed with starting CHT-Core 4.x, using the same environment variables you saved in &lt;code>~/couchdb-single/.env&lt;/code>.&lt;/p>
&lt;h4 id="multi-node">Multi node&lt;/h4>
&lt;p>a) Download 4.x clustered CouchDb docker-compose file:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>mkdir -p ~/couchdb-cluster/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> ~/couchdb-cluster/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -s -o ./docker-compose.yml https://staging.dev.medicmobile.org/_couch/builds_4/medic:medic:&amp;lt;desired CHT version&amp;gt;/docker-compose/cht-couchdb-clustered.yml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>b) Create a data folder for every one of the CouchDb nodes.
If you were going to a 3 cluster node, this would be:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>mkdir -p ~/couchdb-data/main
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mkdir -p ~/couchdb-data/secondary1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mkdir -p ~/couchdb-data/secondary2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>c) Copy the 3.x CouchDb data folder into &lt;code>~/couchdb-data/main&lt;/code>, which will be your main CouchDb node. This main node will create your cluster and the other secondary nodes will be added to it. In &lt;code>main&lt;/code>&amp;rsquo;s environment variable file, define &lt;code>CLUSTER_PEER_IPS&lt;/code>. In all other secondary nodes, declare the &lt;code>COUCHDB_SYNC_ADMINS_NODE&lt;/code> variable instead.&lt;/p>
&lt;p>d) Create a &lt;code>shards&lt;/code> and a &lt;code>.shards&lt;/code> directory in every secondary node folder.&lt;/p>
&lt;p>e) Set the correct environment variables:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>cat &amp;gt; &lt;span style="color:#4e9a06">${&lt;/span>&lt;span style="color:#000">HOME&lt;/span>&lt;span style="color:#4e9a06">}&lt;/span>/couchdb-cluster/.env &lt;span style="color:#4e9a06">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">COUCHDB_USER=&amp;lt;admin&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">COUCHDB_PASSWORD=&amp;lt;password&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">COUCHDB_SECRET=&amp;lt;COUCHDB_SECRET from step 3&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">COUCHDB_UUID=&amp;lt;COUCHDB_UUID from step 3&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">DB1_DATA=&amp;lt;absolute path to main folder created in step 5.a&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">DB2_DATA=&amp;lt;absolute path to secondary1 folder created in step 5.a&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">DB3_DATA=&amp;lt;absolute path to secondary2 folder created in step 5.a&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>f) Start 4.x CouchDb.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> ~/couchdb-cluster/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker-compose up -d
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>g) Update &lt;code>couchdb-migration&lt;/code> environment variables. Depending on your setup, it&amp;rsquo;s possible you will need to update &lt;code>CHT_NETWORK&lt;/code> and &lt;code>COUCH_URL&lt;/code> to match the newly started 4.x CouchDb.
From this point on, the &lt;code>couchdb-migration&lt;/code> container should connect to the same docker network as your CouchDb installation, in order to access APIs that are only available on protected ports. Correctly setting &lt;code>CHT_NETWORK&lt;/code> is required for the next steps to succeed.
To get the correct &lt;code>docker-network-name&lt;/code> and &lt;code>docker-service-name&lt;/code>, you can use &lt;code>docker network ls&lt;/code> to list all networks and &lt;code>docker network inspect &amp;lt;docker-network-name&amp;gt;&lt;/code> to get the name of the CouchDb container that exists in this network.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>cat &amp;gt; &lt;span style="color:#4e9a06">${&lt;/span>&lt;span style="color:#000">HOME&lt;/span>&lt;span style="color:#4e9a06">}&lt;/span>/couchdb-migration/.env &lt;span style="color:#4e9a06">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">CHT_NETWORK=&amp;lt;docker-network-name&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">COUCH_URL=http://&amp;lt;authentication&amp;gt;@&amp;lt;docker-container-name&amp;gt;:&amp;lt;port&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>h) Check that &lt;code>couchdb-migration&lt;/code> can connect to the CouchDb instance and that CouchDb is running. You&amp;rsquo;ll know it is working when the &lt;code>docker-compose&lt;/code> call exits without errors and logs &lt;code>CouchDb Cluster is Ready&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> ~/couchdb-migration/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker-compose run couch-migration check-couchdb-up &amp;lt;number-of-nodes&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>i) Generate the shard distribution matrix and get instructions for final shard locations.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> ~/couchdb-migration/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">shard_matrix&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#204a87;font-weight:bold">$(&lt;/span>docker-compose run couch-migration generate-shard-distribution-matrix&lt;span style="color:#204a87;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker-compose run couch-migration shard-move-instructions &lt;span style="color:#000">$shard_matrix&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>j) Follow the instructions from the step above and move the shard files to the correct location, according to the shard distribution matrix. This is a manual step that requires to physically move data around on disk.&lt;/p>
&lt;p>Example of moving one shard from one node to another:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>/couchdb_data_main
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /.delete
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /_dbs.couch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /_nodes.couch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /_users.couch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /.shards
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /00000000-15555554
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /2aaaaaaa-3ffffffe
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /3fffffff-55555553
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /6aaaaaa9-7ffffffd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /7ffffffe-95555552
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /15555555-2aaaaaa9
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /55555554-6aaaaaa8
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /95555553-aaaaaaa7
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /shards
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /00000000-15555554
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /2aaaaaaa-3ffffffe
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /3fffffff-55555553
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /6aaaaaa9-7ffffffd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /7ffffffe-95555552
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /15555555-2aaaaaa9
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /55555554-6aaaaaa8
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /95555553-aaaaaaa7
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>/couchdb_data_secondary
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /.shards
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /shards
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>After moving two shards: &lt;code>55555554-6aaaaaa8&lt;/code> and &lt;code>6aaaaaa9-7ffffffd&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>/couchdb_data_main
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /.delete
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /_dbs.couch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /_nodes.couch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /_users.couch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /.shards
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /00000000-15555554
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /2aaaaaaa-3ffffffe
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /3fffffff-55555553
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /7ffffffe-95555552
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /15555555-2aaaaaa9
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /95555553-aaaaaaa7
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /shards
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /00000000-15555554
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /2aaaaaaa-3ffffffe
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /3fffffff-55555553
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /7ffffffe-95555552
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /15555555-2aaaaaa9
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /95555553-aaaaaaa7
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>/couchdb_data_secondary
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /.shards
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /6aaaaaa9-7ffffffd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /55555554-6aaaaaa8
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /shards
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /6aaaaaa9-7ffffffd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /55555554-6aaaaaa8
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>k) Change metadata to match the new shard distribution. We declared &lt;code>$shard_matrix&lt;/code> in step &amp;ldquo;g&amp;rdquo; above, so it is still set now:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>docker-compose run couch-migration move-shards &lt;span style="color:#000">$shard_matrix&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>l) Remove old node from the cluster:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>docker-compose run couch-migration remove-node couchdb@127.0.0.1
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>j) Run the &lt;code>verify&lt;/code> command to check whether the migration was successful.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>docker-compose run couch-migration verify
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If all checks pass, you should see a message &lt;code>Migration verification passed&lt;/code>. It is then safe to proceed with starting CHT-Core 4.x, using the same environment variables you saved in &lt;code>~/couchdb-cluster/.env&lt;/code>.&lt;/p></description></item><item><title>Apps: Self Hosting in CHT 4.x</title><link>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/self-hosting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/self-hosting/</guid><description>
&lt;h1 id="recommendations-and-considerations">Recommendations and considerations&lt;/h1>
&lt;h2 id="multi-vs-single-node-couchdb-requirements">Multi vs Single node couchdb requirements&lt;/h2>
&lt;p>For smaller deployments a &lt;a href="https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/self-hosting/single-node/">single node CouchDB&lt;/a> instance can be used, for larger deployments a &lt;a href="https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/self-hosting/multiple-nodes/">multi-node CouchDB&lt;/a> cluster is generally recommended&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Consideration&lt;/th>
&lt;th>&lt;a href="https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/self-hosting/single-node/">Single node CouchDB&lt;/a>&lt;/th>
&lt;th>&lt;a href="https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/self-hosting/multiple-nodes/">Multi-node clustered CouchDB&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Less than
4 000 users&lt;/td>
&lt;td>&lt;span title="Yes">✔&lt;/span>&lt;/td>
&lt;td>&lt;span title="Yes">✔&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>More than
4 000 users&lt;/td>
&lt;td>&lt;span title="No">❌&lt;/span>&lt;/td>
&lt;td>&lt;span title="Yes">✔&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Less than
10 000 documents per day&lt;/td>
&lt;td>&lt;span title="Yes">✔&lt;/span>&lt;/td>
&lt;td>&lt;span title="Yes">✔&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>More than
10 000 documents per day&lt;/td>
&lt;td>&lt;span title="No">❌&lt;/span>&lt;/td>
&lt;td>&lt;span title="Yes">✔&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Seamless upgrade with multi-node docker compose&lt;/td>
&lt;td>&lt;span title="Yes">✔&lt;/span>&lt;/td>
&lt;td>&lt;span title="No">❌&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Seamless upgrade with multi-node kubernetes/k3s&lt;/td>
&lt;td>&lt;span title="Yes">✔&lt;/span>&lt;/td>
&lt;td>&lt;span title="Yes">✔&lt;/span>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="cloud-provider-vs-bare-metal">Cloud provider vs Bare metal&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Consideration&lt;/th>
&lt;th>Cloud provider&lt;/th>
&lt;th>Bare Metal&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Data needs to be in-country&lt;/td>
&lt;td>&lt;span title="No">❌&lt;/span>&lt;/td>
&lt;td>&lt;span title="Yes">✔&lt;/span>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>Apps: k3s - multiple node deployment for VMware</title><link>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/self-hosting-k3s-multinode/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/self-hosting-k3s-multinode/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;p>This page covers an example k3s cluster setup on a VMware datacenter with vSphere 7+ for a national deployment across 50 counties capable of supporting 20,000+ CHWs concurrently. After setup, administrators should only add VMs to the cluster or deploy CHT Core projects to be orchestrated.&lt;/p>
&lt;/div>
&lt;h3 id="about-container-orchestration">About container orchestration&lt;/h3>
&lt;p>A container orchestrator helps easily allocate hardware resources spread across a datacenter. For national scale projects, or a deployments with a large number of CHT Core instances, Medic recommends a lightweight Kubernetes orchestrator called &lt;a href="https://docs.k3s.io/">k3s&lt;/a>. The orchestrator will:&lt;/p>
&lt;ul>
&lt;li>monitor resources across a group of virtual machines (aka &amp;ldquo;nodes&amp;rdquo;)&lt;/li>
&lt;li>place CHT Core projects where there is available resource&lt;/li>
&lt;li>migrate projects to spare resources if combined utilization is high or there are underlying issues.&lt;/li>
&lt;/ul>
&lt;p>Instead of provisioning one VM per CHT Core project, we will provision larger VMs and deploy multiple CHT Core projects on one VM, with each project receiving optional resource limitations, like CPU and RAM.&lt;/p>
&lt;p>In this example an orchestrator is deploying 50 CHT Core projects, one for each county. We will provision 9 large VMs and place 6 CHT Core projects on each VM. This allows for spare resources for failovers and lets the orchestrator decide on which VM projects live. Further, we get automated efficient use of datacenter resource utilization and avoids future manual allocations.&lt;/p>
&lt;h3 id="nodes">Nodes&lt;/h3>
&lt;p>We&amp;rsquo;ll be using two types of k3s nodes in this deployment:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://docs.k3s.io/installation/requirements#cpu-and-memory">HA control-plane&lt;/a> nodes - these enable high availability (HA) and provide access to kube API. These are containers running inside &lt;code>kube-system&lt;/code> namespace which are often associated with the control-plane. They include coreDNS, traefik (ingress), servicelb, VMware Cloud Provisioner Interface (CPI), and VMWare Container Storage Interface (CSI)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Agent or worker nodes - these run the CHT Core containers and projects. They will also run services that tie in networking and storage. VMware CSI-node will be running here which enables agents to mount volumes from VMware Virtual-SAN for block data storage. Agents will also run servicelb-traefik containers which allow the nodes to route traffic to correct projects and handle load-balancing and internal networking.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;h3 id="servers--virtual-machines">Servers / Virtual Machines&lt;/h3>
&lt;p>Provision 3 Ubuntu servers (22.04 as of this writing) that meet k3s specifications for &lt;a href="https://docs.k3s.io/installation/requirements#cpu-and-memory">HA etcd&lt;/a>&lt;/p>
&lt;p>As we&amp;rsquo;re provisioning an example deployment here for 50 counties and over 20,000 CHWs, the RAM, CPU and storage numbers will differ for you specific deployment.&lt;/p>
&lt;p>To support all 50 counties, provision 3 Ubuntu servers (22.04 as of this writing) with &lt;strong>4 vCPU and 8GB Ram&lt;/strong>. Ensure they also meet k3s specifications for &lt;a href="https://docs.k3s.io/installation/requirements#cpu-and-memory">HA etcd&lt;/a>.&lt;/p>
&lt;p>Provision 9 Ubuntu servers (again 22.04 as of this writing) for your k3s agent/worker servers. Each should have &lt;strong>48 vCPU, 192 GB Ram, and 50gb local storage&lt;/strong>.&lt;/p>
&lt;p>For any additional VMs you add to the k3s cluster, you will need to ensure networking, roles, and extra configuration parameters that are noted below are configured on the VM.&lt;/p>
&lt;p>To ensure your hardware is not over-provisioned, add more VMs to your k3s cluster when you want to deploy more CHT Core projects. This gives you flexibility of not needing to provision them initially as they can easily be added later.&lt;/p>
&lt;h3 id="network">Network&lt;/h3>
&lt;p>Ensure the above provisioned VMs:&lt;/p>
&lt;ul>
&lt;li>abide by &lt;a href="https://docs.k3s.io/installation/requirements#inbound-rules-for-k3s-server-nodes">Inbound Rules for k3s Server Nodes&lt;/a>&lt;/li>
&lt;li>If you&amp;rsquo;re using Ubuntu&amp;rsquo;s ufw, follow &lt;a href="https://docs.k3s.io/advanced#ubuntu--debian">firewall considerations for k3s on Ubuntu&lt;/a>&lt;/li>
&lt;li>are restricted to the IP addresses of the k3s nodes so only they can connect to the service ports&lt;/li>
&lt;/ul>
&lt;h3 id="add-roles-and-permissions-to-our-vms">Add Roles and Permissions to our VMs&lt;/h3>
&lt;p>Following the &lt;a href="https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/2.0/vmware-vsphere-csp-getting-started/GUID-0AB6E692-AA47-4B6A-8CEA-38B754E16567.html#GUID-0AB6E692-AA47-4B6A-8CEA-38B754E16567">vSphere docs&lt;/a>, first create the following vSphere roles in vSphere for Container Storage (CSN):&lt;/p>
&lt;ul>
&lt;li>CNS-VM&lt;/li>
&lt;li>CNS-DATASTORE&lt;/li>
&lt;li>CNS-SEARCH-AND-SPBM&lt;/li>
&lt;/ul>
&lt;p>Now, on the VM settings, we can apply these roles as described in the above document.&lt;/p>
&lt;p>Any provisioned VM in the previous step, should recieve CNS-VM role.
The top-level vCenter server will recieve CNS-SEARCH-AND-SPBM role.
Virtual-SAN should recieve CNS-DATASTORE.
And all servers should have the READONLY role (this may already be active)&lt;/p>
&lt;h3 id="enable-necessary-extra-parameters-on-all-vms">Enable Necessary Extra Parameters on all VMs&lt;/h3>
&lt;p>Following along the above document, we want to verify VM Hardware Version is 15 or greater, and that disk.EnableUUID parameter is configured.&lt;/p>
&lt;p>On each node, through vSphere Client (GUI):&lt;/p>
&lt;ol>
&lt;li>
&lt;p>disk.EnableUUID&lt;/p>
&lt;ol>
&lt;li>In the vSphere Client, right-click the VM and select Edit Settings.&lt;/li>
&lt;li>Click the VM Options tab and expand the Advanced menu.&lt;/li>
&lt;li>Click Edit Configuration next to Configuration Parameters.&lt;/li>
&lt;li>Configure the disk.EnableUUID parameter. If the parameter exists, make sure that its value is set to True. If the parameter is not present, add it and set its value to True.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Verify VM hardware version at 15 or higher, and upgrade if necessary&lt;/p>
&lt;ol>
&lt;li>In the vSphere Client, navigate to the virtual machine.&lt;/li>
&lt;li>Select Actions &amp;gt; Compatibility &amp;gt; Upgrade VM Compatibility.&lt;/li>
&lt;li>Click Yes to confirm the upgrade.&lt;/li>
&lt;li>Select a compatibility and click OK.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Add VMware Paravirtual SCSI storage controller to the VM&lt;/p>
&lt;ol>
&lt;li>In the vSphere Client, right-click the VM and select Edit Settings.&lt;/li>
&lt;li>On the Virtual Hardware tab, click the Add New Device button.&lt;/li>
&lt;li>Select SCSI Controller from the drop-down menu.&lt;/li>
&lt;li>Expand New SCSI controller and from the Change Type menu, select VMware Paravirtual.&lt;/li>
&lt;li>Click OK.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;h3 id="identify-vsphere-provider-ids-node-ids-and-datacenter-name">Identify vSphere Provider IDs, Node IDs, and datacenter name&lt;/h3>
&lt;p>Bootstrap parameters for k3s on VMware require UUID identification of each node that will join the cluster.&lt;/p>
&lt;p>For each of the provisioned VMs, you can navigate to the VM in vCenter interface and retrieve the UUID.&lt;/p>
&lt;p>Another method is to make the following calls to vCenter Server API. You may have a VPN that you connect to first before being able to access your vCenter GUI. These commands should be run from the same network that allows that access.&lt;/p>
&lt;p>When running the commands below, be sure to replace the placeholders with your own values:&lt;/p>
&lt;ul>
&lt;li>&lt;code>&amp;lt;vCenter_IP&amp;gt;&lt;/code>&lt;/li>
&lt;li>&lt;code>&amp;lt;USERNAME&amp;gt;&lt;/code>&lt;/li>
&lt;li>&lt;code>&amp;lt;PASSWORD&amp;gt;&lt;/code>&lt;/li>
&lt;li>&lt;code>&amp;lt;UUID_FROM_vCENTER&amp;gt;&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>And any others as well!&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Get an authentication-token:&lt;/p>
&lt;pre tabindex="0">&lt;code>curl -k -X POST https://&amp;lt;vCenter_IP&amp;gt;/rest/com/vmware/cis/session -u &amp;#39;&amp;lt;USERNAME&amp;gt;:&amp;lt;PASSWORD&amp;gt;&amp;#39;
ID=&amp;lt;UUID_FROM_vCENTER&amp;gt;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>List all your VMs and identify the VM-number that was provisioned earlier:&lt;/p>
&lt;pre tabindex="0">&lt;code>curl -k -X GET -H &amp;#34;vmware-api-session-id: $ID&amp;#34; https://&amp;lt;vCenter_IP&amp;gt;/api/vcenter/vm
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Retrieve your instance_uuid by first making a &lt;code>curl&lt;/code> call:&lt;/p>
&lt;pre tabindex="0">&lt;code>curl -k -X GET -H &amp;#34;vmware-api-session-id: $ID&amp;#34; https://&amp;lt;vCenter_IP&amp;gt;/api/vcenter/vm/vm-&amp;lt;number&amp;gt;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Inside the JSON response of the &lt;code>curl&lt;/code> call get the, &lt;code>instance_uuid&lt;/code>, in this case it&amp;rsquo;s &lt;code>215cc603-e8da-5iua-3333-a2402c05121&lt;/code>, but yours will be different:&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;#34;identity&amp;#34;:{&amp;#34;name&amp;#34;:&amp;#34;k3s_worker_node_4&amp;#34;,&amp;#34;instance_uuid&amp;#34;:&amp;#34;215cc603-e8da-5iua-3333-a2402c05121&amp;#34;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Retrieve your datacenter name, to be used in configuration files for VMware CSI and CPI&lt;/p>
&lt;pre tabindex="0">&lt;code>curl -k -X GET -H &amp;#34;vmware-api-session-id: $ID&amp;#34; https://&amp;lt;vCenter_IP&amp;gt;/rest/vcenter/datacenter
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ul>
&lt;p>You will want to save the &amp;ldquo;name&amp;rdquo; of your datacenter.&lt;/p>
&lt;ul>
&lt;li>Retrieve your cluster-id, to be used in config file for VMware CSI
&lt;pre tabindex="0">&lt;code>curl -k -X GET -H &amp;#34;vmware-api-session-id: $ID&amp;#34; https://&amp;lt;vCenter IP&amp;gt;/api/vcenter/cluster
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ul>
&lt;p>You can also use the &lt;a href="https://github.com/vmware/govmomi/blob/main/govc/README.md#binaries">govc cli tool&lt;/a> to retrieve this information:&lt;/p>
&lt;pre tabindex="0">&lt;code>export GOVC_INSECURE=1
export GOVC_URL=&amp;#39;https://&amp;lt;USERNAME&amp;gt;:&amp;lt;PASSWORD&amp;gt;@&amp;lt;vCenter_IP&amp;gt;
govc ls /
&amp;lt;datacenter-name&amp;gt;/vm \
&amp;lt;datacenter-name&amp;gt;/network \
&amp;lt;datacenter-name&amp;gt;/host \
&amp;lt;datacenter-name&amp;gt;/datastore
#To retrieve all Node VMs
govc ls /&amp;lt;datacenter-name&amp;gt;/vm \
&amp;lt;datacenter-name&amp;gt;/vm/&amp;lt;vm-name1&amp;gt; \
&amp;lt;datacenter-name&amp;gt;/vm/&amp;lt;vm-name2&amp;gt; \
&amp;lt;datacenter-name&amp;gt;/vm/&amp;lt;vm-name3&amp;gt; \
&amp;lt;datacenter-name&amp;gt;/vm/&amp;lt;vm-name4&amp;gt; \
&amp;lt;datacenter-name&amp;gt;/vm/&amp;lt;vm-name5&amp;gt;
&lt;/code>&lt;/pre>&lt;h2 id="install-k3s">Install k3s&lt;/h2>
&lt;h3 id="first-control-plane-vm">First Control-Plane VM&lt;/h3>
&lt;p>SSH into your first control-plane VM that was provisioned and configured above and &lt;a href="https://docs.docker.com/engine/install/ubuntu/">install docker&lt;/a>.&lt;/p>
&lt;p>For k3s version compatibiltiy with vCenter and vMware CPI/CSI, we will need to use k3s v1.25, cpi v1.25, and csi v2.7.2 per the &lt;code>curl&lt;/code> call below.&lt;/p>
&lt;p>Run the following CLI command inside the control-plane VM, filling out these two specific values:&lt;/p>
&lt;ul>
&lt;li>&lt;code>&amp;lt;TOKEN&amp;gt;&lt;/code>: Please generate a token ID, and save it. This will be required for the entirety of the k3s cluster existence and required to add additional servers to the k3s cluster&lt;/li>
&lt;li>&lt;code>&amp;lt;VM_UUID&amp;gt;&lt;/code>: This was the UUID for this specific VM that we identified earlier&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=&amp;#34;644&amp;#34; \
INSTALL_K3S_EXEC=&amp;#34;server&amp;#34; INSTALL_K3S_VERSION=&amp;#34;v1.25.14+k3s1&amp;#34; sh -s - \
--docker --token &amp;lt;TOKEN&amp;gt; \
--cluster-init --disable-cloud-controller \
--kubelet-arg=&amp;#34;cloud-provider=external&amp;#34; \
--kubelet-arg=&amp;#34;provider-id=vsphere://&amp;lt;VM_UUID&amp;gt;&amp;#34;
&lt;/code>&lt;/pre>&lt;h3 id="second-and-third-control-plane-vms">Second and third Control-Plane VMs&lt;/h3>
&lt;p>SSH into your second/third control-plane VM.&lt;/p>
&lt;p>Please fill out these values below and run the cli command:&lt;/p>
&lt;ul>
&lt;li>&lt;code>&amp;lt;TOKEN&amp;gt;&lt;/code>: Required to be the same token you used in the first control-plane setup&lt;/li>
&lt;li>&lt;code>&amp;lt;CONTROL_PLANE_1_IP&amp;gt;&lt;/code>: This is the IP of the first control-plane server you setup, and allows this second server to discover the initial one.&lt;/li>
&lt;li>&lt;code>&amp;lt;VM_UUID&amp;gt;&lt;/code>: This is the UUID for this second VM that we identified earlier. This will be different than the one you used for control plane 1.&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=&amp;#34;644&amp;#34; \
INSTALL_K3S_EXEC=&amp;#34;server&amp;#34; INSTALL_K3S_VERSION=&amp;#34;v1.25.14+k3s1&amp;#34; sh -s \
--docker --token &amp;lt;TOKEN&amp;gt; \
--server https://&amp;lt;CONTROL_PLANE_1_IP:6443 \
--disable-cloud-controller --kubelet-arg=&amp;#34;cloud-provider=external&amp;#34; \
--kubelet-arg=&amp;#34;provider-id=vsphere://&amp;lt;VM_UUID&amp;gt;&amp;#34;
&lt;/code>&lt;/pre>&lt;p>You can verify your cluster is working by running this command from inside your control plane VM:&lt;/p>
&lt;pre tabindex="0">&lt;code>/usr/local/bin/k3s kubectl get nodes -o wide
&lt;/code>&lt;/pre>&lt;h3 id="agentworker-vms">Agent/Worker VMs&lt;/h3>
&lt;p>Now we will add our k3s agent/worker servers that will handle cht-core projects, workloads, and containers. This process is the same for any additional Agent/Worker servers you want to add to your k3s cluster.&lt;/p>
&lt;p>Ensure that the appropriate roles, and extra configuration parameters are set correctly.&lt;/p>
&lt;p>Please fill out these values before running the command:&lt;/p>
&lt;ul>
&lt;li>&lt;code>&amp;lt;TOKEN&amp;gt;&lt;/code>: Required to be the same token you used above&lt;/li>
&lt;li>&lt;code>&amp;lt;CONTROL_PLANE_IP&amp;gt;&lt;/code>: The IP of one of the control plane servers you set up above&lt;/li>
&lt;li>&lt;code>&amp;lt;VM_UUID&amp;gt;&lt;/code>: This is the UUID of this VM that we are adding as an agent/worker server in our k3s cluster&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=&amp;#34;644&amp;#34; \
INSTALL_K3S_EXEC=&amp;#34;agent&amp;#34; INSTALL_K3S_VERSION=&amp;#34;v1.25.14+k3s1&amp;#34; sh -s - \
--docker --token &amp;lt;TOKEN&amp;gt; \
--server https://&amp;lt;CONTROL_PLANE_IP&amp;gt;:6443 \
--kubelet-arg=&amp;#34;cloud-provider=external&amp;#34; \
--kubelet-arg=&amp;#34;provider-id=vsphere://&amp;lt;VM_UUID&amp;gt;&amp;#34;
&lt;/code>&lt;/pre>&lt;h2 id="deploy-vmware-cloud-provisioner-interface-cpi-to-your-k3s-cluster">Deploy VMware Cloud Provisioner Interface (CPI) to your k3s cluster&lt;/h2>
&lt;p>SSH into one of your control plane servers.
Download the template for CPI, ensure you are aware of your current working directory. This will be the location where the CPI template is saved.&lt;/p>
&lt;pre tabindex="0">&lt;code>pwd
wget https://raw.githubusercontent.com/kubernetes/cloud-provider-vsphere/release-1.25/releases/v1.25/vsphere-cloud-controller-manager.yaml
&lt;/code>&lt;/pre>&lt;p>Modify the vsphere-cloud-controller-manager.yaml file downloaded above and update vCenter Server information.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Add your &lt;code>&amp;lt;vCenter_IP&amp;gt;&lt;/code> and &lt;code>&amp;lt;USERNAME&amp;gt;&lt;/code>, &lt;code>&amp;lt;PASSWORD&amp;gt;&lt;/code> to the section below inside that yaml:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: v1
kind: Secret
metadata:
name: vsphere-cloud-secret
labels:
vsphere-cpi-infra: secret
component: cloud-controller-manager
namespace: kube-system
# NOTE: this is just an example configuration, update with real values based on your environment
stringData:
&amp;lt;vCenter_IP&amp;gt;.username: &amp;#34;&amp;lt;USERNAME&amp;gt;&amp;#34;
&amp;lt;vCenter_IP&amp;gt;.password: &amp;#34;&amp;lt;PASSWORD&amp;gt;&amp;#34;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Please add your &lt;code>&amp;lt;vCenter_IP&amp;gt;&lt;/code> and &lt;code>&amp;lt;USERNAME&amp;gt;&lt;/code>, &lt;code>&amp;lt;PASSWORD&amp;gt;&lt;/code> and &lt;code>&amp;lt;Datacenter_name_retrieved_earlier&amp;gt;&lt;/code> to the ConfigMap section inside that yaml.&lt;/p>
&lt;p>&lt;strong>Note:&lt;/strong> If your vCenter actively uses https with valid certificates, then inside the &lt;code>global:&lt;/code> stanza, you will want to set &lt;code>insecureFlag: false&lt;/code>. Most set-ups will want this to remain true with&lt;code>insecureFlag: true&lt;/code> .&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: v1
kind: ConfigMap
metadata:
name: vsphere-cloud-config
labels:
vsphere-cpi-infra: config
component: cloud-controller-manager
namespace: kube-system
data:
# NOTE: this is just an example configuration, update with real values based on your environment
vsphere.conf: |
# Global properties in this section will be used for all specified vCenters unless overriden in VirtualCenter section.
global:
port: 443
# set insecureFlag to true if the vCenter uses a self-signed cert
insecureFlag: true
# settings for using k8s secret
secretName: vsphere-cloud-secret
secretNamespace: kube-system
# vcenter section
vcenter:
my-vc-name:
server: &amp;lt;vCenter_IP&amp;gt;
user: &amp;lt;USERNAME&amp;gt;
password: &amp;lt;PASSWORD&amp;gt;
datacenters:
- &amp;lt;Datacenter_name_retrieved_earlier&amp;gt;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Deploy the template!&lt;/p>
&lt;pre tabindex="0">&lt;code>/usr/local/bin/k3s kubectl -n kube-system apply -f vsphere-cloud-controller-manager.yaml
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Verify CPI containers are running:&lt;/p>
&lt;pre tabindex="0">&lt;code>/usr/local/bin/k3s kubectl -n kube-system get pods -o wide
/usr/local/bin/k3s kubectl -n kube-system logs vsphere-cloud-controller-manager-&amp;lt;id&amp;gt;
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ol>
&lt;p>You will see 3 vsphere-cloud-controller-manager pods running, one per control-plane server.&lt;/p>
&lt;p>Take a peak at all 3 vsphere-controller-manager pods logs to ensure nothing is immediately erring. Common errors are using the incorrect datacenter name, UUIDs for VMs in the k3s curl command, or invalid credentials in the configmap and secrets resources created in step 2 above. If one of these errors is displaying in the log, you will want to delete the deployment (in step 3 above, replace &lt;code>apply&lt;/code> with &lt;code>delete&lt;/code>, edit the yaml and re-deploy (run step 3 again).&lt;/p>
&lt;h2 id="deploy-vmware-container-storage-interface-csi-to-your-k3s-cluster">Deploy VMware Container Storage Interface (CSI) to your k3s cluster&lt;/h2>
&lt;p>Follow the &lt;a href="https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/2.0/vmware-vsphere-csp-getting-started/GUID-A1982536-F741-4614-A6F2-ADEE21AA4588.html">VMware documentation for CSI&lt;/a> with these steps:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Run the following command from inside a control-plane server:&lt;/p>
&lt;pre tabindex="0">&lt;code>/usr/local/bin/k3s kubectl create namespace vmware-system-csi
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Taint your control-lane node servers by running the following command. This taint may already exist, if so, thats okay. Please replace &lt;code>&amp;lt;CONTROL_PLANE_SERVER&amp;gt;&lt;/code> with each of your control plane servers.&lt;/p>
&lt;pre tabindex="0">&lt;code>You can retrieve the names by running `/usr/local/bin/k3s kubectl get nodes -o wide`
/usr/local/bin/k3s kubectl taint node &amp;lt;CONTROL_PLANE_SERVER&amp;gt; node-role.kubernetes.io/control-plane=:NoSchedule
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Create kubernetes secret, which will map authentication credentials and datacenter name to CSI containers. First, create a file &lt;code>/etc/kubernetes/csi-vsphere.conf&lt;/code>. Be sure to replace &lt;code>&amp;lt;vCenter_IP&amp;gt;&lt;/code>, &lt;code>&amp;lt;USERNAME&amp;gt;&lt;/code>, &lt;code>&amp;lt;PASSWORD&amp;gt;&lt;/code> , &lt;code>&amp;lt;true_or_false&amp;gt;&lt;/code>, &lt;code>&amp;lt;PORT&amp;gt;&lt;/code> , &lt;code>&amp;lt;datacenter1-path&amp;gt;&lt;/code> and &lt;code>&amp;lt;datacenter1-path&amp;gt;&lt;/code> with your values:&lt;/p>
&lt;pre tabindex="0">&lt;code>[Global]
cluster-id = &amp;#34;&amp;lt;cluster-id&amp;gt;&amp;#34;
[VirtualCenter &amp;#34;&amp;lt;vCenter_IP&amp;gt;&amp;#34;]
insecure-flag = &amp;#34;&amp;lt;true_or_false&amp;gt;&amp;#34;
user = &amp;#34;&amp;lt;USERNAME&amp;gt;&amp;#34;
password = &amp;#34;&amp;lt;PASSWORD&amp;gt;&amp;#34;
port = &amp;#34;&amp;lt;PORT&amp;gt;&amp;#34;
datacenters = &amp;#34;&amp;lt;datacenter1-path&amp;gt;, &amp;lt;datacenter2-path&amp;gt;, ...&amp;#34;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Create the secret resource in the namespace we created in step 1 by running the following command in the same directory you created the csi-vsphere.conf file:&lt;/p>
&lt;pre tabindex="0">&lt;code>/usr/local/bin/k3s kubectl create secret generic vsphere-config-secret --from-file=csi-vsphere.conf --namespace=vmware-system-csi
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Download the &lt;a href="https://raw.githubusercontent.com/kubernetes-sigs/vsphere-csi-driver/v2.7.2/manifests/vanilla/vsphere-csi-driver.yaml">vSphere CSI v2.7.2 template&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>There is one minor edit, typically found on line 217-218, under the deployment specification for vsphere-csi-controller.&lt;/p>
&lt;p>Before edit (original value)&lt;/p>
&lt;pre tabindex="0">&lt;code> nodeSelector:
node-role.kubernetes.io/control-plane: &amp;#34;&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Please add &lt;code>true&lt;/code> as the value for this key, seen below:&lt;/p>
&lt;pre tabindex="0">&lt;code> nodeSelector:
node-role.kubernetes.io/control-plane: &amp;#34;true&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Now, let&amp;rsquo;s deploy VMware CSI by running the following command:&lt;/p>
&lt;pre tabindex="0">&lt;code>/usr/local/bin/k3s kubectl -n vmware-system-csi apply -f vsphere-csi-driver.yaml
&lt;/code>&lt;/pre>&lt;p>Follow the &lt;a href="https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/2.0/vmware-vsphere-csp-getting-started/GUID-54BB79D2-B13F-4673-8CC2-63A772D17B3C.html">verification steps seen here in Step 2 of Procedure&lt;/a>&lt;/p>
&lt;h3 id="create-storageclass-in-k3s-cluster">Create StorageClass in k3s cluster&lt;/h3>
&lt;p>We&amp;rsquo;ll need to create a global &lt;a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">StorageClass&lt;/a> resource in our k3s cluster, so CHT Core deployments will be able to ask for persistent storage volumes from the k3s cluster.&lt;/p>
&lt;p>Inside one of the control-plane servers, please create a file &lt;code>vmware-storageclass.yaml&lt;/code> with the following contents:&lt;/p>
&lt;pre tabindex="0">&lt;code>kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
name: vmware-sc
annotations:
storageclass.kubernetes.io/is-default-class: &amp;#34;true&amp;#34;
provisioner: csi.vsphere.vmware.com
parameters:
csi.storage.k8s.io/fstype: &amp;#34;ext4&amp;#34; #Optional Parameter
&lt;/code>&lt;/pre>&lt;p>Deploy this template to the k3s cluster via:&lt;/p>
&lt;pre tabindex="0">&lt;code>/usr/local/bin/k3s kubectl apply -f vmware-storageclass.yaml
&lt;/code>&lt;/pre>&lt;h2 id="deploying-a-cht-core-project-to-your-new-k3s-cluster-running-on-vmware">Deploying a CHT-Core Project to your new k3s Cluster running on VMware&lt;/h2>
&lt;p>This step will neatly fit into helm chart configurations, but here are the manual steps for time being.&lt;/p>
&lt;p>Your persistent volume (PVC) template for all CouchDB&amp;rsquo;s should be as shown below. Note the &lt;code>storageClassName&lt;/code> parameter should be identical to the &lt;code>storageClass&lt;/code> we deployed earlier:&lt;/p>
&lt;pre tabindex="0">&lt;code># Source: cht-chart/templates/couchdb-n-claim0-persistentvolumeclaim.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
labels:
cht.service: couchdb-1-claim0
name: couchdb-1-claim0
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 4Gi
storageClassName: vmware-sc
status: {}
&lt;/code>&lt;/pre>&lt;h2 id="kubernetes-concepts">Kubernetes Concepts&lt;/h2>
&lt;p>Here are links to docs surrounding the kubernetes concepts that we use in a cht-core project deployed to a k3s cluster.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment&lt;/a> - This is the main kubernetes resource that contains information regarding all the cht services that will be deployed.&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/configuration/configmap/">ConfigMaps&lt;/a> - This contains configuration files, or credentials that containers can retrieve. If you edit the configmap, you should delete containers, which will trigger a new container to download your new edits to any configrations for that service&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/security/service-accounts/">ServiceAccounts&lt;/a> - This is used by the upgrade-service that is running inside the cht-core pods (as a container titled upgrade-service). This serviceAccount restricts the upgrade-service from interacting with any other cht-core projects outside of its namespace, and gives the upgrade-service permissions to talk to kubernetes API to upgrade container images when a CHT ADMIN clicks &lt;em>upgrade&lt;/em> through the Admin interface.&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress&lt;/a> - This is what forwards traffic to a particular project or pods. In most use-cases, there is an nginx deployed outside of the k3s cluster than contains DNS entries for existing projects, and contains a proxy_pass parameter to send traffic based on host header to any of the k3s server IPs. Inside the k3s cluster, the traefik container and servicelb-traefik containers in kube-system namespace will handle forwarding traffic to the correct cht-core containers based on url&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volume Claim&lt;/a> - This is where our project data will be stored. Important to ensure you have configured this correctly, with retain policies intact so the data is not deleted if the project is removed. It&amp;rsquo;s also vital to ensure you have a backup policy either set-up in VMware vCenter GUI or you have configured the csi-snapshotter that comes with vSphere CSI.&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/">Services&lt;/a> - This is utilized for CouchDB nodes to discover each other through DNS rather than internal IPs, which can change. This is also used in the COUCH_URL so API containers can discover where CouchDB is running.&lt;/li>
&lt;/ul></description></item><item><title>Apps: App Developer Hosting in CHT 4.x</title><link>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/app-developer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/app-developer/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;p>This guide assumes you are a CHT app developer wanting to either run concurrent instances of the CHT, or easily be able to switch between different instances without losing any data while doing so. To do development on the CHT Core Framework itself, see the &lt;a href="https://docs.communityhealthtoolkit.org/contribute/code/core/dev-environment/">development guide&lt;/a>.&lt;/p>
&lt;p>To deploy the CHT 3.x in production, see either &lt;a href="https://docs.communityhealthtoolkit.org/apps/guides/hosting/3.x/ec2-setup-guide/">AWS hosting&lt;/a> or &lt;a href="https://docs.communityhealthtoolkit.org/apps/guides/hosting/3.x/self-hosting/">Self hosting&lt;/a>. 4.x production hosting guides are coming soon!&lt;/p>
&lt;/div>
&lt;h2 id="getting-started">Getting started&lt;/h2>
&lt;p>Be sure to meet the &lt;a href="https://docs.communityhealthtoolkit.org/apps/guides/hosting/requirements/">CHT hosting requirements&lt;/a> first. To avoid conflicts, ensure that all other CHT 4.x instances are stopped. To stop ALL containers, you can use&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>docker &lt;span style="color:#204a87">kill&lt;/span> &lt;span style="color:#204a87;font-weight:bold">$(&lt;/span>docker ps -q&lt;span style="color:#204a87;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>After meeting these requirements, create a directory and download the developer YAML files in the directory you want to store them. This example uses &lt;code>~/cht-4-app-developer&lt;/code> as the directory:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>mkdir ~/cht-4-app-developer &lt;span style="color:#ce5c00;font-weight:bold">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#204a87">cd&lt;/span> ~/cht-4-app-developer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -s -o docker-compose.yml https://raw.githubusercontent.com/medic/cht-upgrade-service/main/docker-compose.yml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -s -o cht-core.yml https://staging.dev.medicmobile.org/_couch/builds_4/medic%3Amedic%3Amaster/docker-compose/cht-core.yml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -s -o cht-couchdb.yml https://staging.dev.medicmobile.org/_couch/builds_4/medic%3Amedic%3Amaster/docker-compose/cht-couchdb.yml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You should now have 3 compose files which we can check with &lt;code>ls&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>ls
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cht-core.yml cht-couchdb.yml docker-compose.yml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To start the first developer CHT instance, run &lt;code>docker-compose&lt;/code>, prepending the needed environment variables:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">CHT_COMPOSE_PROJECT_NAME&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>app-devl &lt;span style="color:#000">COUCHDB_SECRET&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>foo &lt;span style="color:#000">DOCKER_CONFIG_PATH&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">${&lt;/span>&lt;span style="color:#000">PWD&lt;/span>&lt;span style="color:#4e9a06">}&lt;/span> &lt;span style="color:#000">COUCHDB_DATA&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">${&lt;/span>&lt;span style="color:#000">PWD&lt;/span>&lt;span style="color:#4e9a06">}&lt;/span>/couchd &lt;span style="color:#000">CHT_COMPOSE_PATH&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">${&lt;/span>&lt;span style="color:#000">PWD&lt;/span>&lt;span style="color:#4e9a06">}&lt;/span> &lt;span style="color:#000">COUCHDB_USER&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>medic &lt;span style="color:#000">COUCHDB_PASSWORD&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>password docker-compose up
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This may take some minutes to fully start depending on the speed of the internet connection and speed of the host. This is because docker needs to download all the storage layers for all the containers and the CHT needs to run the first run set up. After downloads and setup has completed, the CHT should be accessible on &lt;a href="https://localhost">https://localhost&lt;/a>. You can log in with username &lt;code>medic&lt;/code> and password &lt;code>password&lt;/code>.&lt;/p>
&lt;p>When connecting to a new dev CHT instance for the first time, an error will be shown, &amp;ldquo;Your connection is not private&amp;rdquo; (see &lt;a href="https://docs.communityhealthtoolkit.org/apps/tutorials/local-setup/privacy.error.png">screenshot&lt;/a>). To get past this, click &amp;ldquo;Advanced&amp;rdquo; and then click &amp;ldquo;Proceed to localhost&amp;rdquo;.&lt;/p>
&lt;h2 id="running-the-nth-cht-instance">Running the Nth CHT instance&lt;/h2>
&lt;p>After running the first instance of the CHT, it&amp;rsquo;s easy to run as many more as are needed. This is achieved by specifying different:&lt;/p>
&lt;ul>
&lt;li>port for &lt;code>HTTP&lt;/code> redirects (&lt;code>CHT_HTTP&lt;/code>)&lt;/li>
&lt;li>port for &lt;code>HTTPS&lt;/code> traffic (&lt;code>NGINX_HTTP_PORT&lt;/code>)&lt;/li>
&lt;li>directory for storing the compose files and CouchDB files&lt;/li>
&lt;/ul>
&lt;p>Assuming you want to start a new project called &lt;code>the_second&lt;/code> and start the instance on &lt;code>HTTP&lt;/code> port &lt;code>8081&lt;/code> and &lt;code>HTTPS&lt;/code> port &lt;code>8443&lt;/code>, we would first create a new directory and download the same files:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>mkdir ~/the_second &lt;span style="color:#ce5c00;font-weight:bold">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#204a87">cd&lt;/span> ~/the_second
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -s -o docker-compose.yml https://raw.githubusercontent.com/medic/cht-upgrade-service/main/docker-compose.yml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -s -o cht-core.yml https://staging.dev.medicmobile.org/_couch/builds_4/medic%3Amedic%3Amaster/docker-compose/cht-core.yml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -s -o cht-couchdb.yml https://staging.dev.medicmobile.org/_couch/builds_4/medic%3Amedic%3Amaster/docker-compose/cht-couchdb.yml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then, we would use the same &lt;code>docker-compose&lt;/code> command as above, but this time specify the ports:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">NGINX_HTTP_PORT&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">8081&lt;/span> &lt;span style="color:#000">NGINX_HTTPS_PORT&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">8444&lt;/span> &lt;span style="color:#000">CHT_COMPOSE_PROJECT_NAME&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>app-devl &lt;span style="color:#000">COUCHDB_SECRET&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>foo &lt;span style="color:#000">DOCKER_CONFIG_PATH&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">${&lt;/span>&lt;span style="color:#000">PWD&lt;/span>&lt;span style="color:#4e9a06">}&lt;/span> &lt;span style="color:#000">COUCHDB_DATA&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">${&lt;/span>&lt;span style="color:#000">PWD&lt;/span>&lt;span style="color:#4e9a06">}&lt;/span>/couchd &lt;span style="color:#000">CHT_COMPOSE_PATH&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">${&lt;/span>&lt;span style="color:#000">PWD&lt;/span>&lt;span style="color:#4e9a06">}&lt;/span> &lt;span style="color:#000">COUCHDB_USER&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>medic &lt;span style="color:#000">COUCHDB_PASSWORD&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>password docker-compose up
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The second instance is now accessible at &lt;a href="https://localhost:8444">https://localhost:8444&lt;/a> and again using username &lt;code>medic&lt;/code> and password &lt;code>password&lt;/code> to login.&lt;/p>
&lt;h2 id="the-env-file">The &lt;code>.env&lt;/code> file&lt;/h2>
&lt;p>Often times it&amp;rsquo;s convenient to use revision control, like GitHub, to store and publish changes in a CHT app. A nice compliment to this is to store the specifics on how to run the &lt;code>docker-compose&lt;/code> command for each app. By using a shared &lt;code>docker-compose&lt;/code> configuration for all developers on the same app, it avoids any port collisions and enables all developers to have a unified configuration.&lt;/p>
&lt;p>Using the above &lt;code>the_second&lt;/code> sample project, we can create a file &lt;code>~/the_second/.env&lt;/code> with this contents:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">NGINX_HTTP_PORT&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">8081&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">NGINX_HTTPS_PORT&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">8444&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">CHT_COMPOSE_PROJECT_NAME&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>second
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">COUCHDB_SECRET&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>foo
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">DOCKER_CONFIG_PATH&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>./
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">COUCHDB_DATA&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>./couchd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">CHT_COMPOSE_PATH&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>./
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">COUCHDB_USER&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>medic
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">COUCHDB_PASSWORD&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>password
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now it&amp;rsquo;s easy to boot this environment:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> ~/the_second
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker-compose up
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="switching--concurrent-projects">Switching &amp;amp; concurrent projects&lt;/h2>
&lt;p>The easiest way to switch between projects is to stop the first set of containers and start the second set. Cancel the first project running in the foreground with &lt;code>ctrl + c&lt;/code> and &lt;code>stop&lt;/code> all the project&amp;rsquo;s services:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>docker stop second_api_1 second_cht-upgrade-service_1 second_couchdb_1 second_haproxy_1 second_healthcheck_1 second_nginx_1 second_sentinel_1
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Alternately, you can stop ALL containers (even non-CHT ones!) with &lt;code>docker kill $(docker ps -q)&lt;/code>. Then start the other CHT project using either the &lt;code>.env&lt;/code> file or use the explicit command with ports and other environment variables as shown above.&lt;/p>
&lt;p>To run projects concurrently open a second terminal and start the second project so you don&amp;rsquo;t have to cancel and &lt;code>stop&lt;/code> the first project. Remember to avoid port conflicts!&lt;/p>
&lt;h2 id="cht-docker-helper-for-4x">CHT Docker Helper for 4.x&lt;/h2>
&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
This is for CHT 4.x. To use a CHT 3.x version, see the earlier &lt;a href="https://docs.communityhealthtoolkit.org/apps/guides/hosting/3.x/app-developer/#cht-docker-helper">CHT Docker Helper page&lt;/a>
&lt;/div>
&lt;p>The &lt;code>cht-docker-compose.sh&lt;/code> scripts downloads 3 compose files and builds an &lt;code>.env&lt;/code> file used above. This greatly eases starting your first CHT instance with a simple text based GUI which works on Windows (WSL2), macOS (both x86 and Apple Silicon) and Linux.&lt;/p>
&lt;p>&lt;img src="cht-docker-helper.png" alt="The cht-docker-compose.sh script showing the URL and version of the CHT instance as well as number of containers launched, global container count, medic images downloaded count and OS load average. Finally a &amp;amp;ldquo;Successfully started my_first_project&amp;amp;rdquo; message is shown and denotes the login is &amp;amp;ldquo;medic&amp;amp;rdquo; and the password is &amp;amp;ldquo;password&amp;amp;rdquo;.">&lt;/p>
&lt;p>This script brings a lot of benefits with it:&lt;/p>
&lt;ul>
&lt;li>You only have to download one bash script&lt;/li>
&lt;li>All compose files and images will be downloaded automatically for you&lt;/li>
&lt;li>All networks, storage volumes and containers will be created&lt;/li>
&lt;li>A valid TLS certificate will be installed, allowing you to easily test on with CHT Android natively on a mobile device&lt;/li>
&lt;li>An unused port is automatically chosen for you when creating a new project. No more manually looking at your existing &lt;code>.env&lt;/code> files!&lt;/li>
&lt;/ul>
&lt;h3 id="installing">Installing&lt;/h3>
&lt;p>To get started using it:&lt;/p>
&lt;ol>
&lt;li>Clone the &lt;a href="https://github.com/medic/cht-core/">CHT Core&lt;/a> repo&lt;/li>
&lt;li>When you want to check for updates, just run &lt;code>git pull orign&lt;/code> in the &lt;code>cht-core&lt;/code> directory.&lt;/li>
&lt;/ol>
&lt;p>If you want a more stand-alone version, you can &lt;code>curl&lt;/code> the bash script directly, but you can&amp;rsquo;t use &lt;code>git&lt;/code> to easily update it then:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>curl -s -o cht-docker-compose.sh https://raw.githubusercontent.com/medic/cht-core/master/scripts/docker-helper-4.x/cht-docker-compose.sh
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="usage">Usage&lt;/h3>
&lt;p>Always run the script from the directory where it lives. If you launch it from a different directory, relative paths will fail:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Do&lt;/th>
&lt;th>Don&amp;rsquo;t&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>./cht-docker-compose.sh&lt;/code>&lt;/td>
&lt;td>&lt;code>./docker-helper-4.x/cht-docker-compose.sh&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="launching">Launching&lt;/h4>
&lt;p>Run the script with:&lt;/p>
&lt;pre tabindex="0">&lt;code>./cht-docker-compose.sh
&lt;/code>&lt;/pre>&lt;p>The first time you run, you will be prompted to create a new project. Here&amp;rsquo;s what that looks like:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>./cht-docker-compose.sh
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Would you like to initialize a new project &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>y/N&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>? y
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>How &lt;span style="color:#204a87;font-weight:bold">do&lt;/span> you want to name the project? &lt;span style="color:#0000cf;font-weight:bold">4&lt;/span> OH The First
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Downloading compose files ... &lt;span style="color:#204a87;font-weight:bold">done&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Creating network &lt;span style="color:#4e9a06">&amp;#34;4_oh_the_first-cht-net&amp;#34;&lt;/span> with the default driver
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Creating my_first_cht_project-dir_cht-upgrade-service_1 ... &lt;span style="color:#204a87;font-weight:bold">done&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Starting project &lt;span style="color:#4e9a06">&amp;#34;4_oh_the_first&amp;#34;&lt;/span>. First run takes a &lt;span style="color:#204a87;font-weight:bold">while&lt;/span>. Will try &lt;span style="color:#204a87;font-weight:bold">for&lt;/span> up to five minutes........
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --------------------------------------------------------
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Success! &lt;span style="color:#4e9a06">&amp;#34;4_oh_the_first&amp;#34;&lt;/span> is &lt;span style="color:#204a87">set&lt;/span> up:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> https://127-0-0-1.local-ip.medicmobile.org:10444/ &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>CHT&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> https://127-0-0-1.local-ip.medicmobile.org:10444/_utils/ &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>Fauxton&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Login: medic
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Password: password
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --------------------------------------------------------
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Start existing project
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ./cht-docker-compose.sh ENV-FILE.env
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Stop and keep project:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ./cht-docker-compose.sh ENV-FILE.env stop
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Stop and destroy all project data:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ./cht-docker-compose.sh ENV-FILE.env destroy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/app-developer/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Have a great day!
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you have many existing projects, you can specify them to launch them directly. If you had a project called &lt;code>4_oh_the_first&lt;/code> you would run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>./cht-docker-compose.sh 4_oh_the_first.env
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="stopping">Stopping&lt;/h4>
&lt;p>When you&amp;rsquo;re done with a project, it&amp;rsquo;s good to stop all the containers to reduce load on your computer. Do this by specifying the project and the &lt;code>stop&lt;/code> command. This command will simply stop the active Docker containers, and not delete any data. Using our existing example &lt;code>4_oh_the_first&lt;/code> project, you would call:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>./cht-docker-compose.sh 4_oh_the_first.env stop
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="destroying">Destroying&lt;/h4>
&lt;p>When you want to &lt;strong>permanently delete all files and all data&lt;/strong> for a project, specify the project and the &lt;code>destroy&lt;/code> command. Using our existing example &lt;code>4_oh_the_first&lt;/code> project, you would call:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>./cht-docker-compose.sh 4_oh_the_first.env destroy
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Be sure you want to do this, there is no &amp;ldquo;are you sure?&amp;rdquo; prompt and it will delete all your data.&lt;/p>
&lt;p>Also note that this command will use the &lt;code>sudo&lt;/code> command when deleting the CouchDB data, so it may prompt for your password.&lt;/p>
&lt;h3 id="file-locations">File locations&lt;/h3>
&lt;p>The bash script keeps files in two places:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&lt;code>*.env&lt;/code> files&lt;/strong> - the same directory as the bash script.&lt;/li>
&lt;li>&lt;strong>&lt;code>~/medic/cht-docker/&lt;/code> files&lt;/strong> - in your home directory, a sub-directory is created for each project. Within each project directory, a &lt;code>compose&lt;/code> directory has the two compose files and the &lt;code>couch&lt;/code> directory has the CouchDB datafiles.&lt;/li>
&lt;/ul>
&lt;p>While you can manually remove any of these, it&amp;rsquo;s best to use the &lt;code>destroy&lt;/code> command above to ensure all related data files are deleted too.&lt;/p>
&lt;h3 id="video">Video&lt;/h3>
&lt;p>Here is a video of the helper being run on 1 Dec 2022. The video references &lt;code>lazydocker&lt;/code> which is &lt;a href="https://github.com/jesseduffield/lazydocker">a great way&lt;/a> to monitor and control your local docker environment:&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/hrcy8JlJP9M" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;hr></description></item><item><title>Apps: Adding TLS certificates in CHT 4.x</title><link>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/adding-tls-certificates/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/adding-tls-certificates/</guid><description>
&lt;p>By default, CHT 4.x will create a self-signed certificate for every deployment. These instructions are for changing to either a pre-existing certificate or automatically creating and renewing a &lt;a href="https://certbot.eff.org/">Certbot&lt;/a> based certificate using &lt;a href="https://acmeclients.com/">ACME&lt;/a>, like &lt;a href="https://letsencrypt.org/">Let&amp;rsquo;s Encrypt&lt;/a>.&lt;/p>
&lt;p>This guide assumes you&amp;rsquo;ve already met the &lt;a href="https://docs.communityhealthtoolkit.org/apps/guides/hosting/requirements/">hosting requirements&lt;/a>, specifically around Docker being installed.&lt;/p>
&lt;h2 id="pre-existing-certificate">Pre-existing certificate&lt;/h2>
&lt;p>To load your certificates into your CHT instance, we&amp;rsquo;ll be creating an interstitial container called &lt;code>cht-temp-tls&lt;/code> which will enable you to copy your local certificate files into the native docker volume.&lt;/p>
&lt;h3 id="prerequisites">Prerequisites&lt;/h3>
&lt;p>You have two files locally on your workstation in the directory you&amp;rsquo;re currently in:&lt;/p>
&lt;ul>
&lt;li>&lt;code>key.pem&lt;/code> - the private key for your TLS certificate&lt;/li>
&lt;li>&lt;code>chain.pem&lt;/code> - both the public and any interstitial keys concatenated into one file&lt;/li>
&lt;/ul>
&lt;p>Also, be sure you have started your CHT instance once and all your volumes are created.&lt;/p>
&lt;h3 id="loading-the-certificate">Loading the certificate&lt;/h3>
&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
&lt;code>docker compose&lt;/code> should work, but you may need to use the older style &lt;code>docker-compose&lt;/code> if you get an error &lt;code>docker: 'compose' is not a docker command&lt;/code>.
&lt;/div>
&lt;ol>
&lt;li>
&lt;p>Find the name of your &lt;code>cht-ssl&lt;/code> volume with this call:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>docker volume ls --filter &lt;span style="color:#4e9a06">&amp;#34;name=cht-ssl&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It is very likely that &lt;code>cht_cht-ssl&lt;/code> is the name of our &lt;code>cht-ssl&lt;/code> volume.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Using the volume name found in step 1, start a container called &lt;code>temp&lt;/code> which allow us to copy files into the docker volume:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>docker run -d --rm --name temp -v cht_cht-ssl:/etc/nginx/private/ alpine tail -f /dev/null
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Copy the two pem files into the volume via the temporary container:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker cp key.pem temp:/etc/nginx/private/.
docker cp cert.pem temp:/etc/nginx/private/.
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Stop the &lt;code>temp&lt;/code> container:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>docker &lt;span style="color:#204a87">kill&lt;/span> temp
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;p>Your certificates are now safely stored in the native docker volume. Restart your CHT instance the way you started it, being sure to set the correct &lt;code>CERTIFICATE_MODE&lt;/code> and &lt;code>SSL_VOLUME_MOUNT_PATH&lt;/code> per the &lt;a href="#prerequisites">prerequisites&lt;/a>.&lt;/p>
&lt;h2 id="certbot-certificate">Certbot certificate&lt;/h2>
&lt;p>&lt;em>This Feature available on CHT 4.2.0 or later&lt;/em>&lt;/p>
&lt;p>If you have a deployment with a static, public IP and a domain name pointing to that IP, you can have Certbot automatically create free TLS certificates by using &lt;a href="https://hub.docker.com/r/certbot/certbot/">their Docker image&lt;/a>.&lt;/p>
&lt;p>Assuming your CHT instance is running with the default self signed cert. Be sure to change &lt;code>cht.example.com&lt;/code> to your domain first though:&lt;/p>
&lt;p>Assuming your CHT instance is &lt;strong>already running with the default self-signed cert&lt;/strong>:&lt;/p>
&lt;ol>
&lt;li>Edit the CHT&amp;rsquo;s environment file at &lt;code>/home/ubuntu/cht/upgrade-service/.env&lt;/code> so this line is present:
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">CERTIFICATE_MODE&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>AUTO_GENERATE
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>This will ensure the &lt;code>deploy.sh&lt;/code> script that certbot uses to deploy the certificates is available for use.&lt;/li>
&lt;li>Restart your CHT instance to ensure the new &lt;code>CERTIFICATE_MODE&lt;/code> value takes effect:
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> /home/ubuntu/cht/upgrade-service/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker stop &lt;span style="color:#204a87;font-weight:bold">$(&lt;/span>docker ps --filter &lt;span style="color:#4e9a06">&amp;#34;name=^cht*&amp;#34;&lt;/span> -q&lt;span style="color:#204a87;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker stop &lt;span style="color:#204a87;font-weight:bold">$(&lt;/span>docker ps --filter &lt;span style="color:#4e9a06">&amp;#34;name=^upgrade-service*&amp;#34;&lt;/span> -q&lt;span style="color:#204a87;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker compose up --detach
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Create certbot compose and env files by copying and pasting this code:
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>mkdir -p /home/ubuntu/cht/certbot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> /home/ubuntu/cht/certbot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cat &amp;gt; docker-compose.yml &lt;span style="color:#4e9a06">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">version: &amp;#39;3.9&amp;#39;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">services:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> certbot:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> container_name: certbot
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> hostname: certbot
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> image: certbot/certbot
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> volumes:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> - ssl-storage:/etc/nginx/private/
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> - ssl-storage:/var/log/letsencrypt/
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> command: certonly --debug --deploy-hook /etc/nginx/private/deploy.sh --webroot -w /etc/nginx/private/certbot/ --domain \$DOMAIN --non-interactive --key-type rsa --agree-tos --register-unsafely-without-email \$STAGING
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">volumes:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> ssl-storage:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> name: \${CHT_SSL_VOLUME}
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> external: true
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cat &amp;gt; .env &lt;span style="color:#4e9a06">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">DOMAIN=cht.example.com
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">STAGING=
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">CHT_SSL_VOLUME=cht_cht-ssl
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">TZ=America/Whitehorse
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>Certbot only lets you create the identical certificates 5 times per 7 days. If you&amp;rsquo;re unsure of how this works you can change &lt;code>STAGING=&lt;/code> to &lt;code>STAGING=--staging&lt;/code> in the &lt;code>/home/ubuntu/cht/certbot/.env&lt;/code> file to do repeated tests. Be sure to change this back to &lt;code>STAGING=&lt;/code> when you&amp;rsquo;re ready to create production certificates.&lt;/li>
&lt;li>Generate certs:
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> /home/ubuntu/cht/certbot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker compose up
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Run this command to find the name of your CHT ngnix container:
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>docker ps --filter &lt;span style="color:#4e9a06">&amp;#34;name=nginx&amp;#34;&lt;/span> --format &lt;span style="color:#4e9a06">&amp;#39;{{ .Names }}&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Assuming the name is &lt;code>cht_nginx_1&lt;/code> from the prior step, reload your &lt;code>nginx&lt;/code> config with this command:
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>docker &lt;span style="color:#204a87">exec&lt;/span> -it cht_nginx_1 nginx -s reload
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Attempt to renew your certificates once a week by adding this cronjob via &lt;code>crontab -e&lt;/code>. Certbot will only renew them as needed:
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> * * &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> &lt;span style="color:#204a87">cd&lt;/span> /home/ubuntu/cht/certbot&lt;span style="color:#ce5c00;font-weight:bold">&amp;amp;&amp;amp;&lt;/span>docker compose up
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol></description></item><item><title>Apps: Viewing server logs in CHT 4.x</title><link>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/logs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/logs/</guid><description>
&lt;p>CHT 4.x has the following services running via Docker and each can have its logs queried:&lt;/p>
&lt;ul>
&lt;li>nginx&lt;/li>
&lt;li>sentinel&lt;/li>
&lt;li>api&lt;/li>
&lt;li>haproxy&lt;/li>
&lt;li>couchdb&lt;/li>
&lt;li>healthcheck&lt;/li>
&lt;li>upgrade-service&lt;/li>
&lt;/ul>
&lt;h2 id="setting-log-level">Setting log level&lt;/h2>
&lt;p>By default, the CHT server logs are set to the &lt;code>info&lt;/code> level. To change the log level to &lt;code>debug&lt;/code>, you can set the &lt;code>NODE_ENV&lt;/code> environment variable to &lt;code>development&lt;/code>. A log level of &lt;code>debug&lt;/code> can affect system performance and cause log files sizes to grow rapidly. It is recommended to temporarily set the log level to &lt;code>debug&lt;/code> only when needed for troubleshooting.&lt;/p>
&lt;h2 id="viewing-logs">Viewing logs&lt;/h2>
&lt;p>First, find the actual names of the containers with the &lt;code>docker ps --format '{{.Names}}'&lt;/code> command which should show something like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>cht_nginx_1
cht_sentinel_1
cht_api_1
cht_haproxy_1
cht_healthcheck_1
cht_couchdb_1
upgrade-service-cht-upgrade-service-1
&lt;/code>&lt;/pre>&lt;p>You can then use the &lt;code>docker logs&lt;/code> command to view the logs of any given container. For example, if we call &lt;code>docker logs cht_nginx_1&lt;/code> it will show ALL the logs from that container. To show only the last 5 lines, you can use the &lt;code>--tail&lt;/code> flag to specify the number of lines like this &lt;code>docker logs cht_nginx_1 --tail 5&lt;/code>. The result will look like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>10.131.161.1 - - [15/Feb/2023:21:08:35 +0000] &amp;#34;GET /medic/_changes?feed=longpoll&amp;amp;heartbeat=10000&amp;amp;since=115-g1AAAAH5eJyF0LENwjAQBVCLRIAEFBTMgESBCA0lrACJBzgnRXSKoKJmClaAxEswRZbIDCTHZ4GzXPzCT-d_rowx8zIqzDK_3fOycKdkf9jucJIKVyMybmVtxhQp6E-s23jfME00B-LdUWQIOBBxmJkG3gXJHHtfM8WaA2ncQ6RnGmsOZLjG5mLtk2mmSKAUCPHGSkxT3dZAiK_IR28A1AMhzta2wbko2iJe3ndMC92iaIfAzwrTmn8as5aY&amp;amp;limit=25 HTTP/1.1&amp;#34; 499 0 &amp;#34;https://10-131-161-159.local-ip.medicmobile.org/&amp;#34; &amp;#34;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0&amp;#34;
10.131.161.1 - - [15/Feb/2023:21:08:35 +0000] &amp;#34;GET /medic-user-medic-meta/_changes?include_docs=true&amp;amp;feed=longpoll&amp;amp;heartbeat=10000&amp;amp;since=13-g1AAAAH5eJyF0LENwkAMhWETKGkoWIICERpKWAESD3BOiugUQUXNFKwAiZdgiiyRGUjMY4FYV_zFfbJ8VxPRspqXtCpu96Iq5ZTuD9sdTlrjKgkka-Y8htkE-hOWjWrrOBCVo9noOBATzMwcB5JLVG0cB9LKw2xwHMh4XdCF-TktgTIg5I0nubYBQr5mnxiSaTsAIWfmzp2LRTvkpdq7Fov2CH7WYOMP5CCWMg&amp;amp;limit=25 HTTP/1.1&amp;#34; 499 0 &amp;#34;https://10-131-161-159.local-ip.medicmobile.org/&amp;#34; &amp;#34;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0&amp;#34;
10.131.161.1 - - [15/Feb/2023:21:08:35 +0000] &amp;#34;GET /fontawesome-webfont.woff2 HTTP/1.1&amp;#34; 304 0 &amp;#34;https://10-131-161-159.local-ip.medicmobile.org/styles.css&amp;#34; &amp;#34;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0&amp;#34;
10.131.161.1 - - [15/Feb/2023:21:08:35 +0000] &amp;#34;GET /fonts/NotoSans-Bold.ttf HTTP/1.1&amp;#34; 304 0 &amp;#34;https://10-131-161-159.local-ip.medicmobile.org/styles.css&amp;#34; &amp;#34;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0&amp;#34;
10.131.161.1 - - [15/Feb/2023:21:08:35 +0000] &amp;#34;GET /fonts/NotoSans-Regular.ttf HTTP/1.1&amp;#34; 200 221787 &amp;#34;https://10-131-161-159.local-ip.medicmobile.org/styles.css&amp;#34; &amp;#34;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0
&lt;/code>&lt;/pre>&lt;p>Sometimes you may want to search the logs for a specific string. To search, use the pipe (&lt;code>|&lt;/code>) and &lt;code>grep&lt;/code> commands to do this. Here we search for all the times HA Proxy thought CouchDB wasn&amp;rsquo;t reachable (&lt;code>DOWN&lt;/code>) with this call &lt;code>docker logs cht_haproxy_1 2&amp;gt;&amp;amp;1 | grep 'DOWN'&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;lt;145&amp;gt;Feb 15 20:52:06 haproxy[25]: Server couchdb-servers/couchdb is DOWN, reason: Layer7 wrong status, code: 0, info: &amp;#34;via agent : down&amp;#34;, check duration: 208ms. 0 active and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue.
[WARNING] 045/205206 (25) : Server couchdb-servers/couchdb is DOWN, reason: Layer7 wrong status, code: 0, info: &amp;#34;via agent : down&amp;#34;, check duration: 208ms. 0 active and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue.
[WARNING] 045/205601 (25) : Server couchdb-servers/couchdb is DOWN, reason: Layer7 wrong status, code: 0, info: &amp;#34;via agent : down&amp;#34;, check duration: 207ms. 0 active and 0 backup servers left. 5 sessions active, 0 requeued, 0 remaining in queue.
&amp;lt;145&amp;gt;Feb 15 20:56:01 haproxy[25]: Server couchdb-servers/couchdb is DOWN, reason: Layer7 wrong status, code: 0, info: &amp;#34;via agent : down&amp;#34;, check duration: 207ms. 0 active and 0 backup servers left. 5 sessions active, 0 requeued, 0 remaining in queue.
&lt;/code>&lt;/pre>&lt;p>If you want to watch the logs for a specific container in real time, you can use the &lt;code>--follow&lt;/code> flag. This command would watch the requests come into API in realtime: &lt;code>docker logs cht_api_1 --follow&lt;/code>. It&amp;rsquo;s nice to couple this with the &lt;code>--tail&lt;/code> command so you only see the last 5 lines of the existing logs before watching for new lines with &lt;code>docker logs cht_api_1 --follow --tail 5&lt;/code> which would show this:&lt;/p>
&lt;pre tabindex="0">&lt;code>RES d17d71f5-2dcb-4ebb-bb0e-7874b3000570 10.131.161.1 - GET /medic/_design/medic-client/_view/reports_by_subject?keys=%5B%22557e79b8-2d99-4bd1-a4d6-a44491d483d8%22%5D HTTP/1.0 200 - 12.452 ms
RES e43c5d7f-4e32-433a-a96d-ef991f4298a3 10.131.161.1 - GET /medic/_design/medic/_view/doc_summaries_by_id?keys=%5B%22557e79b8-2d99-4bd1-a4d6-a44491d483d8%22%5D HTTP/1.0 200 - 31.226 ms
REQ c656ecc7-e6af-4564-ad63-2cab2c42844a 10.131.161.1 - GET /medic/_all_docs?include_docs=true&amp;amp;startkey=%22target~2023-02~557e79b8-2d99-4bd1-a4d6-a44491d483d8~%22&amp;amp;endkey=%22target~2023-02~557e79b8-2d99-4bd1-a4d6-a44491d483d8~%EF%BF%B0%22 HTTP/1.0
RES c656ecc7-e6af-4564-ad63-2cab2c42844a 10.131.161.1 - GET /medic/_all_docs?include_docs=true&amp;amp;startkey=%22target~2023-02~557e79b8-2d99-4bd1-a4d6-a44491d483d8~%22&amp;amp;endkey=%22target~2023-02~557e79b8-2d99-4bd1-a4d6-a44491d483d8~%EF%BF%B0%22 HTTP/1.0 200 - 11.153 ms
2023-02-15 21:54:49 DEBUG: Checking for a configured outgoing message service
&lt;/code>&lt;/pre></description></item><item><title>Apps: Backups in CHT 4.x</title><link>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/backups/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/backups/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;p>This guide is about backups in CHT 4.x - there&amp;rsquo;s the &lt;a href="https://docs.communityhealthtoolkit.org/apps/guides/hosting/3.x/self-hosting/#backup">self hosted guide for 3.x&lt;/a> which includes backups for 3.x.&lt;/p>
&lt;/div>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>As CHT 4.x uses a container per service, the only data that needs to be backed up is:&lt;/p>
&lt;ul>
&lt;li>CouchDB database&lt;/li>
&lt;li>Docker Compose and &lt;code>.env&lt;/code> files&lt;/li>
&lt;li>TLS certificates&lt;/li>
&lt;/ul>
&lt;p>This is because Docker containers are inherently stateless so all the important binaries are already stored in &lt;a href="https://gallery.ecr.aws/s5s3h4s7/">CHT&amp;rsquo;s Docker images&lt;/a>. Docker Compose files, including the &lt;code>.env&lt;/code> file, store all of your deployment&amp;rsquo;s configuration. Finally, the TLS certificates should be backed up to reduce recovery time.&lt;/p>
&lt;p>How to backup each of these three pieces of data is covered below.&lt;/p>
&lt;p>Therefore, you do &lt;strong>not&lt;/strong> need to back up the docker images for:&lt;/p>
&lt;ul>
&lt;li>nginx&lt;/li>
&lt;li>sentinel&lt;/li>
&lt;li>api&lt;/li>
&lt;li>haproxy&lt;/li>
&lt;li>couchdb&lt;/li>
&lt;li>healthcheck&lt;/li>
&lt;li>upgrade-service&lt;/li>
&lt;/ul>
&lt;h2 id="assumptions">Assumptions&lt;/h2>
&lt;p>This guide assumes you have an Ubuntu server running CHT 4.x in Docker as described in our &lt;a href="https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/self-hosting/single-node/">Self Hosting in CHT 4.x - Single CouchDB Node&lt;/a> guide. If you run &lt;code>docker ps --format '{{.Names}}'&lt;/code> you should see something like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>cht_nginx_1
cht_sentinel_1
cht_api_1
cht_haproxy_1
cht_healthcheck_1
cht_couchdb_1
upgrade-service-cht-upgrade-service-1
&lt;/code>&lt;/pre>&lt;p>If you run &lt;code>docker volume ls&lt;/code> you should see something like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>DRIVER VOLUME NAME
local cht_cht-credentials
local cht_cht-ssl
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Note&lt;/strong> - In the volumes listed above, there is no volume for CouchDB data. This is because the compose file declares this as a &lt;a href="https://docs.docker.com/storage/bind-mounts/">bind mount&lt;/a>. Bind mounts use the host file system directly and do not show up in &lt;code>docker volume ls&lt;/code> calls. It&amp;rsquo;s therefore assumed your CouchDB data location is declared in &lt;code>/home/ubuntu/cht/upgrade-service/.env&lt;/code> which sets it with &lt;code>COUCHDB_DATA=/home/ubuntu/cht/couchdb&lt;/code>.&lt;/p>
&lt;p>You should have SSH access to the server with &lt;code>root&lt;/code> access.&lt;/p>
&lt;h3 id="backup-software">Backup software&lt;/h3>
&lt;p>It&amp;rsquo;s assumed you are using which ever tool you&amp;rsquo;re familiar with which might include &lt;a href="https://rsync.samba.org/examples.html">rsync&lt;/a>, &lt;a href="https://borgbackup.readthedocs.io/en/stable/">borg&lt;/a>, &lt;a href="https://duplicity.gitlab.io/">duplicity&lt;/a> or other solution. The locations of the backups should follow the 3-2-1 rule:&lt;/p>
&lt;blockquote>
&lt;p>There should be at least 3 copies of the data, stored on 2 different types of storage media, and one copy should be kept offsite, in a remote location. &lt;em>- &lt;a href="https://en.wikipedia.org/wiki/Backup">Wikipedia&lt;/a>&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>Duplicity has the handy benefit of offering built in encryption using &lt;a href="https://gnupg.org/">GPG&lt;/a>. Consider using this if you don&amp;rsquo;t have an existing solution for encrypted backups.&lt;/p>
&lt;h2 id="couchdb">CouchDB&lt;/h2>
&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
CouchDB backups, by necessity, will have PII and PHI. They should be safely stored to prevent unauthorized access including encrypting backups.
&lt;/div>
&lt;p>Assuming your CouchDB is stored in &lt;code>/home/ubuntu/cht/couchdb&lt;/code>, you should use these steps to back it up:&lt;/p>
&lt;ol>
&lt;li>While you don&amp;rsquo;t need to stop CouchDB to back it up, ensure you follow best practices to back it up. See the &lt;a href="https://docs.couchdb.org/en/stable/maintenance/backups.html">CouchDB site&lt;/a> for more info. Note that Medic recommends NOT using replication for backup.&lt;/li>
&lt;li>It is strongly recommended you encrypt your backups given the sensitivity of the contents. Do this now before copying the backup files to their long term location.&lt;/li>
&lt;li>Backup the CouchDB files using the &lt;a href="#backup-software">software specified above&lt;/a>&lt;/li>
&lt;/ol>
&lt;h2 id="docker-compose-files">Docker Compose files&lt;/h2>
&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
The &lt;code>.env&lt;/code> file contains cleartext passwords. It should be safely stored to prevent unauthorized access.
&lt;/div>
&lt;p>All compose files, and the corresponding &lt;code>.env&lt;/code> file, are in these three locations:&lt;/p>
&lt;ul>
&lt;li>/home/ubuntu/cht/compose/*.yml&lt;/li>
&lt;li>/home/ubuntu/cht/upgrade-service/*.yml&lt;/li>
&lt;li>/home/ubuntu/cht/upgrade-service/.env&lt;/li>
&lt;/ul>
&lt;p>While all three of these are trivial to recreate by downloading them again, they may change over time so should be archived with your CouchDB data. Further, when there&amp;rsquo;s been a critical failure of a production CHT instance, you want to be sure to make the restore process as speedy as possible.&lt;/p>
&lt;p>As all of these files are only read when Docker first loads a service, you can simply copy them whenever you want without stopping any of the CHT services. They should be copied with the same frequency and put in the same location as the CouchDB data using the &lt;a href="#backup-software">backup software specified above&lt;/a>.&lt;/p>
&lt;h2 id="tls-certificates">TLS certificates&lt;/h2>
&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
The &lt;code>.key&lt;/code> file is the private key for TLS certificate. It should be safely stored to prevent unauthorized access.
&lt;/div>
&lt;p>Like the compose files, the TLS certificate files can easily be regenerated or re-downloaded from your Certificate Authority, like Let&amp;rsquo;s Encrypt for example. However, you want to have a backup of the at the ready to ease the restore process.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Copy the cert and key files from the nginx container:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>docker cp cht_nginx_1:/etc/nginx/private/key.pem .
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker cp cht_nginx_1:/etc/nginx/private/cert.pem .
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Back the up to the same location and frequency as the CouchDB data using the &lt;a href="#backup-software">backup software specified above&lt;/a>.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="testing-backups">Testing backups&lt;/h2>
&lt;p>A backup that isn&amp;rsquo;t tested, is effectively not a backup. For a backup to be successful, a complete restore from all locations in the 3-2-1 plan need to be fully tested and documented as to how a restore works. The more practiced and better documented the restore process, the less downtime a production CHT instance will have after data loss.&lt;/p></description></item><item><title>Apps: Docker Directory Setup</title><link>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/_partial_docker_directories/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://docs.communityhealthtoolkit.org/apps/guides/hosting/4.x/_partial_docker_directories/</guid><description>
&lt;p>Create the following directory structure:&lt;/p>
&lt;pre tabindex="0">&lt;code>|-- /home/ubuntu/cht/
|-- compose/
|-- certs/
|-- couchdb/
|-- upgrade-service/
&lt;/code>&lt;/pre>&lt;p>By calling this &lt;code>mkdir&lt;/code> commands:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>mkdir -p /home/ubuntu/cht/&lt;span style="color:#ce5c00;font-weight:bold">{&lt;/span>compose,certs,upgrade-service,couchdb&lt;span style="color:#ce5c00;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol>
&lt;li>&lt;code>compose&lt;/code> - docker-compose files for cht-core and CouchDB&lt;/li>
&lt;li>&lt;code>certs&lt;/code> - TLS certificates directory&lt;/li>
&lt;li>&lt;code>upgrade-service&lt;/code> - where docker-compose file for the upgrade-service&lt;/li>
&lt;li>&lt;code>couchdb&lt;/code> - the path for the docker-compose file of the upgrade-service (not used in multi-node)&lt;/li>
&lt;/ol></description></item></channel></rss>